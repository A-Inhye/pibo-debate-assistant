# WhisperLiveKit í•œêµ­ì–´ ìƒì„¸ ë¬¸ì„œ

## ğŸ“‹ ëª©ì°¨
1. [í”„ë¡œì íŠ¸ ê°œìš”](#1-í”„ë¡œì íŠ¸-ê°œìš”)
2. [ì „ì²´ ì•„í‚¤í…ì²˜](#2-ì „ì²´-ì•„í‚¤í…ì²˜)
3. [íŒŒì¼ êµ¬ì¡°](#3-íŒŒì¼-êµ¬ì¡°)
4. [í•µì‹¬ ëª¨ë“ˆ ì„¤ëª…](#4-í•µì‹¬-ëª¨ë“ˆ-ì„¤ëª…)
5. [ë°ì´í„° íë¦„](#5-ë°ì´í„°-íë¦„)
6. [ì£¼ìš” ì•Œê³ ë¦¬ì¦˜](#6-ì£¼ìš”-ì•Œê³ ë¦¬ì¦˜)
7. [ì„¤ì¹˜ ë° ì‚¬ìš©ë²•](#7-ì„¤ì¹˜-ë°-ì‚¬ìš©ë²•)
8. [ì„±ëŠ¥ ìµœì í™”](#8-ì„±ëŠ¥-ìµœì í™”)
9. [ë°°í¬ ê°€ì´ë“œ](#9-ë°°í¬-ê°€ì´ë“œ)

---

## 1. í”„ë¡œì íŠ¸ ê°œìš”

### 1.1 WhisperLiveKitì´ë€?

**WhisperLiveKit**ì€ ì´ˆì €ì§€ì—° ì‹¤ì‹œê°„ ìŒì„±-í…ìŠ¤íŠ¸ ë³€í™˜(STT) ì‹œìŠ¤í…œì…ë‹ˆë‹¤. í™”ì ì‹ë³„(Speaker Diarization) ê¸°ëŠ¥ê³¼ 200ê°œ ì–¸ì–´ ë²ˆì—­ì„ ì§€ì›í•˜ëŠ” ìì²´ í˜¸ìŠ¤íŒ… ê°€ëŠ¥í•œ ì†”ë£¨ì…˜ì…ë‹ˆë‹¤.

### 1.2 ì£¼ìš” íŠ¹ì§•

- âš¡ **ì´ˆì €ì§€ì—°**: 300-800ms ë‹¨ì–´ ë‹¨ìœ„ ì§€ì—°ì‹œê°„
- ğŸ¯ **ì‹¤ì‹œê°„ ìŠ¤íŠ¸ë¦¬ë°**: ë°œí™”ì™€ ë™ì‹œì— í…ìŠ¤íŠ¸ ì¶œë ¥
- ğŸ‘¥ **í™”ì ì‹ë³„**: ì—¬ëŸ¬ í™”ìë¥¼ ì‹¤ì‹œê°„ìœ¼ë¡œ êµ¬ë¶„
- ğŸŒ **ë‹¤êµ­ì–´ ì§€ì›**: 99ê°œ ì–¸ì–´ ì¸ì‹, 200ê°œ ì–¸ì–´ ë²ˆì—­
- ğŸ”’ **í”„ë¼ì´ë²„ì‹œ**: ìì²´ ì„œë²„ í˜¸ìŠ¤íŒ…, ë°ì´í„° ì™¸ë¶€ ì „ì†¡ ì—†ìŒ
- ğŸš€ **ë‹¤ì¤‘ ë°±ì—”ë“œ**: PyTorch, Faster-Whisper, MLX, OpenAI API

### 1.3 ê¸°ìˆ  ìŠ¤íƒ

| ê³„ì¸µ | ê¸°ìˆ  |
|------|------|
| **AI ëª¨ë¸** | OpenAI Whisper (ìŒì„±ì¸ì‹), NLLB (ë²ˆì—­), Sortformer/Diart (í™”ìì‹ë³„) |
| **ë°±ì—”ë“œ** | Python 3.9+, FastAPI, asyncio, WebSocket |
| **ì¶”ë¡  ì—”ì§„** | PyTorch, Faster-Whisper (CTranslate2), MLX-Whisper |
| **ì˜¤ë””ì˜¤ ì²˜ë¦¬** | FFmpeg, librosa, soundfile, Silero VAD |
| **í”„ë¡ íŠ¸ì—”ë“œ** | HTML5, WebSocket API, AudioWorklet |

### 1.4 í”„ë¡œì íŠ¸ í†µê³„

```
ì´ ì½”ë“œ ë¼ì¸: 11,457 LOC (Python)
ë²„ì „: 0.2.17.post1
ë¼ì´ì„¼ìŠ¤: MIT / Apache 2.0
ì§€ì› Python: 3.9 - 3.15
```

---

## 2. ì „ì²´ ì•„í‚¤í…ì²˜

### 2.1 ì‹œìŠ¤í…œ êµ¬ì„±ë„

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        ë¸Œë¼ìš°ì € í´ë¼ì´ì–¸íŠ¸                      â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”‚
â”‚  â”‚ MediaRecorderâ”‚  â”‚ AudioWorklet â”‚  â”‚  WebSocket   â”‚      â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
          â”‚ WebM/Opus        â”‚ PCM s16le        â”‚
          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                    WebSocket ë°”ì´ë„ˆë¦¬ ìŠ¤íŠ¸ë¦¼
                              â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    FastAPI WebSocket ì„œë²„                   â”‚
â”‚                    (basic_server.py)                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   AudioProcessor (í•µì‹¬ íŒŒì´í”„ë¼ì¸)           â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚ FFmpegManagerâ”‚â†’ â”‚  Silero VAD  â”‚â†’ â”‚  ì˜¤ë””ì˜¤ í   â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                                 â”‚
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚                                             â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                 â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚  SimulStreaming ASR  â”‚                 â”‚  LocalAgreement ASR     â”‚
        â”‚  (AlignAtt ì •ì±…)     â”‚                 â”‚  (ë²„í¼ ê¸°ë°˜ ì •ì±…)        â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â”‚                                             â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                  â”‚ ASRToken ìŠ¤íŠ¸ë¦¼
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚    í™”ì ì‹ë³„ (ì„ íƒ)          â”‚
                    â”‚  â€¢ Sortformer (SOTA 2025) â”‚
                    â”‚  â€¢ Diart (SOTA 2021)      â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                  â”‚ SpeakerSegment ìŠ¤íŠ¸ë¦¼
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚    ë²ˆì—­ (ì„ íƒ)              â”‚
                    â”‚  â€¢ NLLW (200ê°œ ì–¸ì–´)       â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                  â”‚
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚   TokensAlignment         â”‚
                    â”‚  (í† í° + í™”ì + ë²ˆì—­ ë³‘í•©)  â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                  â”‚ Segment ê°ì²´
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚  JSON ì‘ë‹µ (FrontData)     â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                  â”‚
                          WebSocket.send_json()
                                  â”‚
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚   ë¸Œë¼ìš°ì € UI ì—…ë°ì´íŠ¸      â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 2.2 ì£¼ìš” ì»´í¬ë„ŒíŠ¸

#### 2.2.1 TranscriptionEngine (ì½”ì–´ ì—”ì§„)
- **ì—­í• **: ëª¨ë“  AI ëª¨ë¸ì„ ì´ˆê¸°í™”í•˜ê³  ê´€ë¦¬í•˜ëŠ” ì‹±ê¸€í†¤ í´ë˜ìŠ¤
- **íŒ¨í„´**: ìŠ¤ë ˆë“œ ì•ˆì „ ì‹±ê¸€í†¤ (Double-Checked Locking)
- **ê´€ë¦¬ ëŒ€ìƒ**: ASR ë°±ì—”ë“œ, í™”ì ì‹ë³„, ë²ˆì—­, VAD

#### 2.2.2 AudioProcessor (ì˜¤ë””ì˜¤ íŒŒì´í”„ë¼ì¸)
- **ì—­í• **: ì‹¤ì‹œê°„ ì˜¤ë””ì˜¤ ìŠ¤íŠ¸ë¦¼ ì²˜ë¦¬ ë° ê²°ê³¼ ì¡°í•©
- **íŠ¹ì§•**: asyncio ê¸°ë°˜ ë¹„ë™ê¸° ì²˜ë¦¬, ì—¬ëŸ¬ íë¥¼ í†µí•œ ë³‘ë ¬ ì²˜ë¦¬
- **ì²˜ë¦¬ íë¦„**: FFmpeg ë””ì½”ë”© â†’ VAD â†’ ASR â†’ í™”ìì‹ë³„ â†’ ì •ë ¬

#### 2.2.3 SimulStreamingASR (ìµœì‹  ë°±ì—”ë“œ)
- **ì—­í• **: AlignAtt ì •ì±…ì„ ì‚¬ìš©í•œ ë™ì‹œ ìŠ¤íŠ¸ë¦¬ë° ASR
- **í•µì‹¬ ê¸°ìˆ **: Attention Alignment Headsë¡œ ë‹¨ì–´ ê²½ê³„ ì˜ˆì¸¡
- **ì§€ì—°ì‹œê°„**: 300-800ms ë‹¨ì–´ ë‹¨ìœ„

#### 2.2.4 LocalAgreement (ì•ˆì •ì  ë°±ì—”ë“œ)
- **ì—­í• **: ë²„í¼ ê¸°ë°˜ ê°€ì„¤ ë§¤ì¹­ìœ¼ë¡œ ì•ˆì •ì  ì¶œë ¥
- **í•µì‹¬ ê¸°ìˆ **: Hypothesis Bufferì™€ Longest Common Prefix
- **ì§€ì—°ì‹œê°„**: 1-3ì´ˆ ë¬¸ì¥ ë‹¨ìœ„

---

## 3. íŒŒì¼ êµ¬ì¡°

### 3.1 ì „ì²´ ë””ë ‰í† ë¦¬ êµ¬ì¡°

```
WhisperLiveKit-main/
â”œâ”€â”€ whisperlivekit/                      # ë©”ì¸ íŒ¨í‚¤ì§€ (11,457 LOC)
â”‚   â”œâ”€â”€ __init__.py                      # íŒ¨í‚¤ì§€ ì§„ì…ì 
â”‚   â”œâ”€â”€ core.py                          # TranscriptionEngine (213 LOC)
â”‚   â”œâ”€â”€ audio_processor.py               # AudioProcessor (635 LOC)
â”‚   â”œâ”€â”€ basic_server.py                  # FastAPI ì„œë²„ (131 LOC)
â”‚   â”œâ”€â”€ parse_args.py                    # CLI ì¸ì íŒŒì„œ (333 LOC)
â”‚   â”œâ”€â”€ backend_support.py               # ë°±ì—”ë“œ ê°ì§€ (42 LOC)
â”‚   â”œâ”€â”€ model_paths.py                   # ëª¨ë¸ ê²½ë¡œ ê´€ë¦¬ (203 LOC)
â”‚   â”œâ”€â”€ thread_safety.py                 # ìŠ¤ë ˆë“œ ì•ˆì „ì„± (31 LOC)
â”‚   â”œâ”€â”€ timed_objects.py                 # ë°ì´í„° êµ¬ì¡° (229 LOC)
â”‚   â”œâ”€â”€ tokens_alignment.py              # í† í° ì •ë ¬ (220 LOC)
â”‚   â”œâ”€â”€ warmup.py                        # ëª¨ë¸ ì›Œë°ì—… (51 LOC)
â”‚   â”œâ”€â”€ ffmpeg_manager.py                # FFmpeg ê´€ë¦¬ (198 LOC)
â”‚   â”œâ”€â”€ silero_vad_iterator.py           # ìŒì„± í™œë™ ê°ì§€ (326 LOC)
â”‚   â”‚
â”‚   â”œâ”€â”€ simul_whisper/                   # SimulStreaming ë°±ì—”ë“œ (2,152 LOC)
â”‚   â”‚   â”œâ”€â”€ simul_whisper.py             # AlignAtt ë””ì½”ë” (720 LOC)
â”‚   â”‚   â”œâ”€â”€ backend.py                   # ASR ì„¤ì • (259 LOC)
â”‚   â”‚   â”œâ”€â”€ config.py                    # ì„¤ì • í´ë˜ìŠ¤ (80 LOC)
â”‚   â”‚   â”œâ”€â”€ decoder_state.py             # ë””ì½”ë” ìƒíƒœ (197 LOC)
â”‚   â”‚   â”œâ”€â”€ token_buffer.py              # í† í° ë²„í¼ (141 LOC)
â”‚   â”‚   â”œâ”€â”€ beam.py                      # ë¹” íƒìƒ‰ (280 LOC)
â”‚   â”‚   â”œâ”€â”€ eow_detection.py             # ë‹¨ì–´ ë ê°ì§€ (157 LOC)
â”‚   â”‚   â”œâ”€â”€ mlx_encoder.py               # MLX ì¸ì½”ë” (156 LOC)
â”‚   â”‚   â””â”€â”€ mlx/                         # MLX ë³€í˜• (162 LOC)
â”‚   â”‚
â”‚   â”œâ”€â”€ local_agreement/                 # LocalAgreement ë°±ì—”ë“œ (1,025 LOC)
â”‚   â”‚   â”œâ”€â”€ whisper_online.py            # ASR íŒ©í† ë¦¬ (92 LOC)
â”‚   â”‚   â”œâ”€â”€ online_asr.py                # ì˜¨ë¼ì¸ í”„ë¡œì„¸ì„œ (377 LOC)
â”‚   â”‚   â””â”€â”€ backends.py                  # ë°±ì—”ë“œ êµ¬í˜„ (556 LOC)
â”‚   â”‚
â”‚   â”œâ”€â”€ diarization/                     # í™”ì ì‹ë³„ (505 LOC)
â”‚   â”‚   â”œâ”€â”€ diart_backend.py             # Diart ë°±ì—”ë“œ (233 LOC)
â”‚   â”‚   â””â”€â”€ sortformer_backend.py        # Sortformer ë°±ì—”ë“œ (272 LOC)
â”‚   â”‚
â”‚   â”œâ”€â”€ whisper/                         # Whisper êµ¬í˜„ (7,031 LOC)
â”‚   â”‚   â”œâ”€â”€ model.py                     # Whisper ëª¨ë¸ (407 LOC)
â”‚   â”‚   â”œâ”€â”€ audio.py                     # ì˜¤ë””ì˜¤ ì „ì²˜ë¦¬ (157 LOC)
â”‚   â”‚   â”œâ”€â”€ tokenizer.py                 # í† í¬ë‚˜ì´ì € (395 LOC)
â”‚   â”‚   â”œâ”€â”€ decoding.py                  # ë””ì½”ë” (821 LOC)
â”‚   â”‚   â”œâ”€â”€ transcribe.py                # ì „ì‚¬ íŒŒì´í”„ë¼ì¸ (608 LOC)
â”‚   â”‚   â”œâ”€â”€ timing.py                    # íƒ€ì´ë° ì¶”ì¶œ (145 LOC)
â”‚   â”‚   â”œâ”€â”€ utils.py                     # ìœ í‹¸ë¦¬í‹° (213 LOC)
â”‚   â”‚   â”œâ”€â”€ triton_ops.py                # Triton ìµœì í™” (121 LOC)
â”‚   â”‚   â””â”€â”€ normalizers/                 # í…ìŠ¤íŠ¸ ì •ê·œí™” (534 LOC)
â”‚   â”‚
â”‚   â”œâ”€â”€ web/                             # ì›¹ ì¸í„°í˜ì´ìŠ¤ (325 LOC + í”„ë¡ íŠ¸ì—”ë“œ)
â”‚   â”‚   â”œâ”€â”€ web_interface.py             # HTML/CSS/JS ë¡œë”
â”‚   â”‚   â”œâ”€â”€ live_transcription.html      # ì›¹ UI
â”‚   â”‚   â”œâ”€â”€ live_transcription.css       # ìŠ¤íƒ€ì¼
â”‚   â”‚   â”œâ”€â”€ live_transcription.js        # í”„ë¡ íŠ¸ì—”ë“œ ë¡œì§
â”‚   â”‚   â”œâ”€â”€ pcm_worklet.js               # AudioWorklet í”„ë¡œì„¸ì„œ
â”‚   â”‚   â””â”€â”€ recorder_worker.js           # Web Worker
â”‚   â”‚
â”‚   â””â”€â”€ silero_vad_models/               # ì‚¬ì „ í›ˆë ¨ëœ VAD ëª¨ë¸
â”‚       â”œâ”€â”€ silero_vad.onnx              # ONNX ëª¨ë¸ (opset 16)
â”‚       â””â”€â”€ silero_vad.jit               # JIT ëª¨ë¸
â”‚
â”œâ”€â”€ scripts/                             # ìœ í‹¸ë¦¬í‹° ìŠ¤í¬ë¦½íŠ¸
â”‚   â”œâ”€â”€ convert_hf_whisper.py            # HuggingFace ëª¨ë¸ ë³€í™˜
â”‚   â”œâ”€â”€ determine_alignment_heads.py     # Alignment Head ì¶”ì¶œ
â”‚   â””â”€â”€ sync_extension.py                # Chrome í™•ì¥ ë™ê¸°í™”
â”‚
â”œâ”€â”€ chrome-extension/                    # ë¸Œë¼ìš°ì € í™•ì¥ í”„ë¡œê·¸ë¨
â”‚   â”œâ”€â”€ background.js
â”‚   â”œâ”€â”€ sidepanel.js
â”‚   â”œâ”€â”€ requestPermissions.js
â”‚   â””â”€â”€ manifest.json
â”‚
â”œâ”€â”€ docs/                                # ë¬¸ì„œ
â”‚   â”œâ”€â”€ API.md
â”‚   â”œâ”€â”€ technical_integration.md
â”‚   â”œâ”€â”€ troubleshooting.md
â”‚   â””â”€â”€ supported_languages.md
â”‚
â”œâ”€â”€ pyproject.toml                       # í”„ë¡œì íŠ¸ ë©”íƒ€ë°ì´í„°
â”œâ”€â”€ README.md                            # ë©”ì¸ ë¬¸ì„œ
â”œâ”€â”€ Dockerfile                           # GPU ì§€ì› ë„ì»¤
â””â”€â”€ Dockerfile.cpu                       # CPU ì „ìš© ë„ì»¤
```

### 3.2 í•µì‹¬ íŒŒì¼ ì„¤ëª…

| íŒŒì¼ | LOC | ì£¼ìš” ê¸°ëŠ¥ |
|------|-----|-----------|
| `core.py` | 213 | ì‹±ê¸€í†¤ ì—”ì§„, ëª¨ë¸ ì´ˆê¸°í™”, íŒ©í† ë¦¬ í•¨ìˆ˜ |
| `audio_processor.py` | 635 | ì˜¤ë””ì˜¤ íŒŒì´í”„ë¼ì¸, ë¹„ë™ê¸° ì²˜ë¦¬, ê²°ê³¼ ì¡°í•© |
| `basic_server.py` | 131 | FastAPI ì„œë²„, WebSocket ì—”ë“œí¬ì¸íŠ¸ |
| `simul_whisper/simul_whisper.py` | 720 | AlignAtt ë””ì½”ë”, KV-ìºì‹œ ê´€ë¦¬ |
| `whisper/model.py` | 407 | Whisper íŠ¸ëœìŠ¤í¬ë¨¸ ì•„í‚¤í…ì²˜ |
| `whisper/decoding.py` | 821 | ë¹” íƒìƒ‰, ê·¸ë¦¬ë”” ë””ì½”ë”© |
| `whisper/transcribe.py` | 608 | ì „ì‚¬ íŒŒì´í”„ë¼ì¸ |
| `ffmpeg_manager.py` | 198 | FFmpeg ë¹„ë™ê¸° ê´€ë¦¬ |
| `tokens_alignment.py` | 220 | í¬ë¡œìŠ¤ëª¨ë‹¬ ì •ë ¬ |

---

## 4. í•µì‹¬ ëª¨ë“ˆ ì„¤ëª…

### 4.1 core.py - TranscriptionEngine

#### 4.1.1 ì‹±ê¸€í†¤ íŒ¨í„´ êµ¬í˜„

```python
class TranscriptionEngine:
    _instance = None
    _initialized = False
    _lock = threading.Lock()  # ìŠ¤ë ˆë“œ ì•ˆì „ ì ê¸ˆ

    def __new__(cls, *args, **kwargs):
        # Double-Checked Locking íŒ¨í„´
        if cls._instance is None:
            with cls._lock:
                if cls._instance is None:
                    cls._instance = super().__new__(cls)
        return cls._instance
```

**ëª©ì **: ì—¬ëŸ¬ WebSocket ì—°ê²°ì—ì„œ ë™ì¼í•œ ëª¨ë¸ ì¸ìŠ¤í„´ìŠ¤ë¥¼ ê³µìœ í•˜ì—¬ ë©”ëª¨ë¦¬ ì ˆì•½

#### 4.1.2 ì´ˆê¸°í™” í”„ë¡œì„¸ìŠ¤

```python
def __init__(self, **kwargs):
    with TranscriptionEngine._lock:
        if TranscriptionEngine._initialized:
            return  # ì´ë¯¸ ì´ˆê¸°í™”ë¨
        TranscriptionEngine._initialized = True

    # 1. ê¸°ë³¸ íŒŒë¼ë¯¸í„° ì„¤ì •
    global_params = {
        "host": "localhost",
        "port": 8000,
        "diarization": False,
        "target_language": "",
        "backend_policy": "simulstreaming",
        "backend": "auto",
    }

    # 2. ASR ë°±ì—”ë“œ ì„ íƒ
    if backend_policy == "simulstreaming":
        self.asr = SimulStreamingASR(...)
    else:
        self.asr = backend_factory(backend="faster-whisper", ...)

    # 3. í™”ì ì‹ë³„ ë¡œë“œ (ì„ íƒ)
    if diarization:
        self.diarization_model = SortformerDiarization() or DiartDiarization()

    # 4. ë²ˆì—­ ëª¨ë¸ ë¡œë“œ (ì„ íƒ)
    if target_language:
        self.translation_model = load_model([lan], backend="ctranslate2")
```

#### 4.1.3 íŒ©í† ë¦¬ í•¨ìˆ˜

```python
def online_factory(args, asr):
    """ê° ì—°ê²°ë§ˆë‹¤ ìƒˆë¡œìš´ ì˜¨ë¼ì¸ í”„ë¡œì„¸ì„œ ìƒì„±"""
    if args.backend_policy == "simulstreaming":
        return SimulStreamingOnlineProcessor(asr)
    return OnlineASRProcessor(asr)

def online_diarization_factory(args, diarization_backend):
    """ê° ì—°ê²°ë§ˆë‹¤ ìƒˆë¡œìš´ í™”ì ì‹ë³„ ì¸ìŠ¤í„´ìŠ¤ ìƒì„±"""
    if args.diarization_backend == "sortformer":
        return SortformerDiarizationOnline(shared_model=diarization_backend)
    return diarization_backend  # DiartëŠ” ê³µìœ 
```

---

### 4.2 audio_processor.py - AudioProcessor

#### 4.2.1 ì£¼ìš” ì»´í¬ë„ŒíŠ¸

```python
class AudioProcessor:
    def __init__(self, **kwargs):
        # ì˜¤ë””ì˜¤ ì„¤ì •
        self.sample_rate = 16000  # 16kHz
        self.channels = 1  # ëª¨ë…¸

        # ë¹„ë™ê¸° í
        self.transcription_queue = asyncio.Queue()  # ASRìš©
        self.diarization_queue = asyncio.Queue()    # í™”ì ì‹ë³„ìš©
        self.translation_queue = asyncio.Queue()    # ë²ˆì—­ìš©

        # FFmpeg ê´€ë¦¬ì (WebM/Opus â†’ PCM ë³€í™˜)
        self.ffmpeg_manager = FFmpegManager(
            sample_rate=16000,
            channels=1
        )

        # Silero VAD (ìŒì„± í™œë™ ê°ì§€)
        self.vac = FixedVADIterator(load_jit_vad())

        # ì˜¨ë¼ì¸ í”„ë¡œì„¸ì„œ (ì—°ê²°ë³„ ì¸ìŠ¤í„´ìŠ¤)
        self.transcription = online_factory(args, models.asr)
        self.diarization = online_diarization_factory(args, models.diarization_model)
        self.translation = online_translation_factory(args, models.translation_model)
```

#### 4.2.2 ë¹„ë™ê¸° ì‘ì—… ìƒì„±

```python
async def create_tasks(self):
    """ëª¨ë“  ì²˜ë¦¬ ì‘ì—…ì„ ë³‘ë ¬ë¡œ ì‹œì‘"""
    # 1. FFmpeg stdout ì½ê¸°
    self.ffmpeg_reader_task = asyncio.create_task(
        self.ffmpeg_stdout_reader()
    )

    # 2. ì „ì‚¬ í”„ë¡œì„¸ì„œ
    self.transcription_task = asyncio.create_task(
        self.transcription_processor()
    )

    # 3. í™”ì ì‹ë³„ í”„ë¡œì„¸ì„œ
    self.diarization_task = asyncio.create_task(
        self.diarization_processor()
    )

    # 4. ë²ˆì—­ í”„ë¡œì„¸ì„œ
    self.translation_task = asyncio.create_task(
        self.translation_processor()
    )

    # 5. ê²°ê³¼ í¬ë§·í„° (ì œë„ˆë ˆì´í„°)
    return self.results_formatter()
```

#### 4.2.3 ì˜¤ë””ì˜¤ ì²˜ë¦¬ íë¦„

```python
async def process_audio(self, message: bytes):
    """WebSocketì—ì„œ ë°›ì€ ì˜¤ë””ì˜¤ ë©”ì‹œì§€ ì²˜ë¦¬"""
    if not message:
        # ë¹ˆ ë©”ì‹œì§€ = ìŠ¤íŠ¸ë¦¼ ì¢…ë£Œ ì‹ í˜¸
        await self.transcription_queue.put(SENTINEL)
        return

    if self.is_pcm_input:
        # PCM ëª¨ë“œ: ì§ì ‘ ë²„í¼ì— ì¶”ê°€
        self.pcm_buffer.extend(message)
        await self.handle_pcm_data()
    else:
        # ì••ì¶• ëª¨ë“œ: FFmpegë¡œ ì „ì†¡
        await self.ffmpeg_manager.write_data(message)
```

#### 4.2.4 VAD ì²˜ë¦¬

```python
async def handle_pcm_data(self):
    """PCM ë°ì´í„° ì²˜ë¦¬ ë° VAD ì ìš©"""
    pcm_array = np.frombuffer(self.pcm_buffer, dtype=np.int16).astype(np.float32) / 32768.0

    # Silero VAD ì‹¤í–‰
    res = self.vac(pcm_array)

    if res is not None:
        if "start" in res:
            # ìŒì„± ì‹œì‘ ê°ì§€
            await self._end_silence()

        if "end" in res:
            # ìŒì„± ì¢…ë£Œ ê°ì§€
            pre_silence_chunk = self._slice_before_silence(pcm_array, res["end"])
            await self._enqueue_active_audio(pre_silence_chunk)
            await self._begin_silence()

    # ìŒì„± í™œë™ ì¤‘ì´ë©´ íì— ì¶”ê°€
    if not self.current_silence:
        await self._enqueue_active_audio(pcm_array)
```

#### 4.2.5 ì „ì‚¬ í”„ë¡œì„¸ì„œ

```python
async def transcription_processor(self):
    """ASR ë°±ì—”ë“œ í˜¸ì¶œ ë° í† í° ìƒì„±"""
    while True:
        item = await get_all_from_queue(self.transcription_queue)

        if item is SENTINEL:
            break  # ìŠ¤íŠ¸ë¦¼ ì¢…ë£Œ

        if isinstance(item, Silence):
            # ì¹¨ë¬µ ì²˜ë¦¬
            new_tokens, processed_upto = await asyncio.to_thread(
                self.transcription.start_silence
            )
            self.transcription.end_silence(item.duration)

        elif isinstance(item, np.ndarray):
            # ì˜¤ë””ì˜¤ ì²­í¬ ì²˜ë¦¬
            stream_time = len(item) / self.sample_rate
            self.transcription.insert_audio_chunk(item, stream_time)

            # ASR ì‹¤í–‰ (ë³„ë„ ìŠ¤ë ˆë“œì—ì„œ)
            new_tokens, processed_upto = await asyncio.to_thread(
                self.transcription.process_iter
            )

        # ìƒíƒœ ì—…ë°ì´íŠ¸ (ìŠ¤ë ˆë“œ ì•ˆì „)
        async with self.lock:
            self.state.tokens.extend(new_tokens)
            self.state.buffer_transcription = self.transcription.get_buffer()
            self.state.end_buffer = max(processed_upto, self.state.end_buffer)
            self.state.new_tokens.extend(new_tokens)

        # ë²ˆì—­ íì— í† í° ì „ë‹¬
        if self.translation_queue:
            for token in new_tokens:
                await self.translation_queue.put(token)
```

#### 4.2.6 ê²°ê³¼ í¬ë§·í„°

```python
async def results_formatter(self):
    """ì²˜ë¦¬ ê²°ê³¼ë¥¼ í”„ë¡ íŠ¸ì—”ë“œ í˜•ì‹ìœ¼ë¡œ ë³€í™˜"""
    while True:
        # TokensAlignment ì—…ë°ì´íŠ¸
        self.tokens_alignment.update()

        # ì •ë ¬ëœ ì„¸ê·¸ë¨¼íŠ¸ ìƒì„±
        lines, buffer_diarization, buffer_translation = self.tokens_alignment.get_lines(
            diarization=self.args.diarization,
            translation=bool(self.translation)
        )

        # í˜„ì¬ ìƒíƒœ ê°€ì ¸ì˜¤ê¸°
        state = await self.get_current_state()

        # JSON ì‘ë‹µ ìƒì„±
        response = FrontData(
            status="active_transcription",
            lines=lines,  # Segment ë¦¬ìŠ¤íŠ¸
            buffer_transcription=state.buffer_transcription.text,
            buffer_diarization=buffer_diarization,
            buffer_translation=buffer_translation,
            remaining_time_transcription=state.remaining_time_transcription,
            remaining_time_diarization=state.remaining_time_diarization
        )

        # ë³€ê²½ì‚¬í•­ì´ ìˆì„ ë•Œë§Œ ì „ì†¡
        if response != self.last_response_content:
            yield response
            self.last_response_content = response

        await asyncio.sleep(0.05)  # 20 FPS
```

---

### 4.3 simul_whisper/simul_whisper.py - AlignAtt

#### 4.3.1 AlignAtt ë””ì½”ë” ê°œìš”

**AlignAtt**ëŠ” Attention Alignment Headsë¥¼ ì‚¬ìš©í•˜ì—¬ ë‹¨ì–´ ê²½ê³„ë¥¼ ì˜ˆì¸¡í•˜ëŠ” ë™ì‹œ ìŠ¤íŠ¸ë¦¬ë° ë””ì½”ë”ì…ë‹ˆë‹¤.

#### 4.3.2 í•µì‹¬ ë©”ì»¤ë‹ˆì¦˜

```python
class AlignAtt:
    def __init__(self, model, alignment_heads, frame_threshold=25):
        self.model = model  # Whisper ëª¨ë¸
        self.alignment_heads = alignment_heads  # [(layer, head), ...]
        self.frame_threshold = frame_threshold  # ë°œí™” ì„ê³„ê°’
        self.decoder_state = DecoderState()

    def decode_streaming(self, encoder_output, audio_chunk):
        """ìŠ¤íŠ¸ë¦¬ë° ë””ì½”ë”©"""
        # 1. ë””ì½”ë” ì‹¤í–‰
        logits = self.model.decoder(
            self.decoder_state.tokens,
            encoder_output,
            kv_cache=self.decoder_state.kv_cache
        )

        # 2. Attention Alignment ê³„ì‚°
        alignment_scores = self.compute_alignment(
            encoder_output,
            self.decoder_state.tokens,
            self.alignment_heads
        )

        # 3. ë°œí™” ì§€ì  ê°ì§€
        if self.should_fire(alignment_scores, self.frame_threshold):
            # ë‹¤ìŒ í† í° ìƒ˜í”Œë§
            next_token = self.sample_token(logits[-1])

            # ë””ì½”ë” ìƒíƒœ ì—…ë°ì´íŠ¸
            self.decoder_state.tokens.append(next_token)
            self.decoder_state.kv_cache = self.update_kv_cache()

            # ASRToken ìƒì„±
            return self.create_asr_token(next_token, alignment_scores)

        return None  # ì•„ì§ ë°œí™”í•˜ì§€ ì•ŠìŒ
```

#### 4.3.3 Attention Alignment Heads

```python
def compute_alignment(self, encoder_output, tokens, alignment_heads):
    """
    Cross-Attention ê°€ì¤‘ì¹˜ë¥¼ ì‚¬ìš©í•˜ì—¬ ì •ë ¬ ê³„ì‚°

    Returns:
        alignment_scores: [num_tokens, num_frames] í˜•íƒœì˜ í…ì„œ
    """
    all_attentions = []

    for layer_idx, head_idx in alignment_heads:
        # íŠ¹ì • ë ˆì´ì–´ì™€ í—¤ë“œì˜ attention ê°€ì¤‘ì¹˜ ì¶”ì¶œ
        attention_weights = self.model.decoder.blocks[layer_idx].cross_attn.attn_weights[head_idx]
        # shape: [num_tokens, num_frames]
        all_attentions.append(attention_weights)

    # í‰ê·  attention
    alignment_scores = torch.stack(all_attentions).mean(dim=0)
    return alignment_scores

def should_fire(self, alignment_scores, threshold):
    """
    ë°œí™” ì§€ì  ê°ì§€

    Args:
        alignment_scores: [num_tokens, num_frames]
        threshold: 25 (default) - 25% í™•ë¥ ë¡œ ë°œí™”

    Returns:
        bool: True if should fire
    """
    # ë§ˆì§€ë§‰ í† í°ì˜ attention ë¶„í¬
    last_token_attention = alignment_scores[-1]  # [num_frames]

    # ì˜¤ë””ì˜¤ì˜ ë§ˆì§€ë§‰ 25ê°œ í”„ë ˆì„ì— ëŒ€í•œ attention í•©
    num_frames_to_check = threshold  # 25 frames = 0.5ì´ˆ
    attention_on_recent = last_token_attention[-num_frames_to_check:].sum()

    # ì„ê³„ê°’ ì´ˆê³¼ ì‹œ ë°œí™”
    return attention_on_recent > 0.25  # 25% ì´ìƒ
```

#### 4.3.4 KV-Cache ê´€ë¦¬

```python
def update_kv_cache(self):
    """
    KV-Cacheë¥¼ ì—…ë°ì´íŠ¸í•˜ì—¬ ë©”ëª¨ë¦¬ ì ˆì•½

    KV-CacheëŠ” ì´ì „ í† í°ë“¤ì˜ Key/Value í…ì„œë¥¼ ì €ì¥í•˜ì—¬
    ë‹¤ìŒ í† í° ìƒì„± ì‹œ ì¬ê³„ì‚°ì„ ë°©ì§€
    """
    new_cache = {}

    for layer_name, (key, value) in self.decoder_state.kv_cache.items():
        # ìµœëŒ€ ì»¨í…ìŠ¤íŠ¸ ê¸¸ì´ ìœ ì§€ (ì˜ˆ: 448 í† í°)
        if len(self.decoder_state.tokens) > self.max_context_tokens:
            # ì˜¤ë˜ëœ í† í°ì˜ KV ì œê±°
            key = key[:, -self.max_context_tokens:]
            value = value[:, -self.max_context_tokens:]

        new_cache[layer_name] = (key, value)

    return new_cache

def clean_cache(self):
    """GPU ë©”ëª¨ë¦¬ í•´ì œ"""
    for key in list(self.decoder_state.kv_cache.keys()):
        tensor = self.decoder_state.kv_cache.pop(key, None)
        if tensor is not None:
            del tensor

    if torch.cuda.is_available():
        torch.cuda.empty_cache()
```

---

### 4.4 whisper/model.py - Whisper ì•„í‚¤í…ì²˜

#### 4.4.1 Whisper ëª¨ë¸ êµ¬ì¡°

```python
class Whisper(nn.Module):
    def __init__(self, dims: ModelDimensions):
        super().__init__()

        # ì˜¤ë””ì˜¤ ì¸ì½”ë” (80-dim mel â†’ 1280-dim features)
        self.encoder = AudioEncoder(
            n_mels=dims.n_mels,  # 80
            n_ctx=dims.n_audio_ctx,  # 1500 (30ì´ˆ)
            n_state=dims.n_audio_state,  # 1280
            n_head=dims.n_audio_head,  # 20
            n_layer=dims.n_audio_layer  # 32 (large ëª¨ë¸)
        )

        # í…ìŠ¤íŠ¸ ë””ì½”ë” (í† í° â†’ ë‹¤ìŒ í† í° ì˜ˆì¸¡)
        self.decoder = TextDecoder(
            n_vocab=dims.n_vocab,  # 51865 (ë‹¤êµ­ì–´)
            n_ctx=dims.n_text_ctx,  # 448
            n_state=dims.n_text_state,  # 1280
            n_head=dims.n_text_head,  # 20
            n_layer=dims.n_text_layer  # 32
        )

    def forward(self, mel, tokens):
        # ì¸ì½”ë”: mel-spectrogram â†’ features
        encoder_output = self.encoder(mel)

        # ë””ì½”ë”: tokens + encoder_output â†’ logits
        logits = self.decoder(tokens, encoder_output)

        return logits
```

#### 4.4.2 AudioEncoder

```python
class AudioEncoder(nn.Module):
    def __init__(self, n_mels, n_ctx, n_state, n_head, n_layer):
        super().__init__()

        # Conv1d ë ˆì´ì–´ (80-dim â†’ 1280-dim)
        self.conv1 = Conv1d(n_mels, n_state, kernel_size=3, padding=1)
        self.conv2 = Conv1d(n_state, n_state, kernel_size=3, stride=2, padding=1)

        # Positional Embedding
        self.positional_embedding = nn.Parameter(torch.empty(n_ctx, n_state))

        # Transformer ë¸”ë¡ (32ê°œ)
        self.blocks = nn.ModuleList([
            ResidualAttentionBlock(n_state, n_head)
            for _ in range(n_layer)
        ])

        self.ln_post = LayerNorm(n_state)

    def forward(self, x):
        # x: [batch, n_mels=80, n_frames=3000]

        # Conv ë ˆì´ì–´ í†µê³¼
        x = F.gelu(self.conv1(x))
        x = F.gelu(self.conv2(x))
        # x: [batch, n_state=1280, n_frames=1500]

        x = x.permute(0, 2, 1)  # [batch, n_frames=1500, n_state=1280]

        # Positional Embedding ì¶”ê°€
        x = (x + self.positional_embedding).to(x.dtype)

        # Transformer ë¸”ë¡ í†µê³¼
        for block in self.blocks:
            x = block(x)

        x = self.ln_post(x)
        # x: [batch, n_frames=1500, n_state=1280]

        return x
```

#### 4.4.3 TextDecoder

```python
class TextDecoder(nn.Module):
    def __init__(self, n_vocab, n_ctx, n_state, n_head, n_layer):
        super().__init__()

        # Token Embedding
        self.token_embedding = nn.Embedding(n_vocab, n_state)

        # Positional Embedding
        self.positional_embedding = nn.Parameter(torch.empty(n_ctx, n_state))

        # Transformer ë¸”ë¡ (Self-Attention + Cross-Attention)
        self.blocks = nn.ModuleList([
            ResidualAttentionBlock(
                n_state,
                n_head,
                cross_attention=True  # Cross-Attention í™œì„±í™”
            )
            for _ in range(n_layer)
        ])

        self.ln = LayerNorm(n_state)

    def forward(self, tokens, encoder_output, kv_cache=None):
        # tokens: [batch, seq_len]
        # encoder_output: [batch, n_frames=1500, n_state=1280]

        # Token Embedding
        x = self.token_embedding(tokens)
        # x: [batch, seq_len, n_state=1280]

        # Positional Embedding
        x = x + self.positional_embedding[:tokens.shape[-1]]

        # Transformer ë¸”ë¡ í†µê³¼
        for i, block in enumerate(self.blocks):
            x = block(
                x,
                xa=encoder_output,  # Cross-Attentionì˜ Key/Value
                kv_cache=kv_cache.get(i) if kv_cache else None
            )

        x = self.ln(x)
        # x: [batch, seq_len, n_state=1280]

        # Output Projection (n_state â†’ n_vocab)
        logits = (x @ torch.transpose(self.token_embedding.weight.to(x.dtype), 0, 1)).float()
        # logits: [batch, seq_len, n_vocab=51865]

        return logits
```

---

### 4.5 ffmpeg_manager.py - FFmpegManager

#### 4.5.1 FFmpeg ìƒíƒœ ê´€ë¦¬

```python
class FFmpegState(Enum):
    STOPPED = "stopped"
    STARTING = "starting"
    RUNNING = "running"
    RESTARTING = "restarting"
    FAILED = "failed"

class FFmpegManager:
    def __init__(self, sample_rate=16000, channels=1):
        self.sample_rate = sample_rate
        self.channels = channels
        self.state = FFmpegState.STOPPED
        self._state_lock = asyncio.Lock()
```

#### 4.5.2 FFmpeg í”„ë¡œì„¸ìŠ¤ ì‹œì‘

```python
async def start(self):
    """FFmpeg ì„œë¸Œí”„ë¡œì„¸ìŠ¤ ì‹œì‘"""
    async with self._state_lock:
        if self.state != FFmpegState.STOPPED:
            return False
        self.state = FFmpegState.STARTING

    try:
        # FFmpeg ëª…ë ¹ì–´ êµ¬ì„±
        cmd = [
            "ffmpeg",
            "-hide_banner",
            "-loglevel", "error",
            "-i", "pipe:0",  # stdinì—ì„œ ì…ë ¥
            "-f", "s16le",  # PCM signed 16-bit little-endian
            "-acodec", "pcm_s16le",
            "-ac", str(self.channels),  # 1 (ëª¨ë…¸)
            "-ar", str(self.sample_rate),  # 16000 Hz
            "pipe:1"  # stdoutìœ¼ë¡œ ì¶œë ¥
        ]

        # ë¹„ë™ê¸° ì„œë¸Œí”„ë¡œì„¸ìŠ¤ ìƒì„±
        self.process = await asyncio.create_subprocess_exec(
            *cmd,
            stdin=asyncio.subprocess.PIPE,
            stdout=asyncio.subprocess.PIPE,
            stderr=asyncio.subprocess.PIPE
        )

        # stderr ë¡œê¹… ì‘ì—… ì‹œì‘
        self._stderr_task = asyncio.create_task(self._drain_stderr())

        async with self._state_lock:
            self.state = FFmpegState.RUNNING

        return True

    except FileNotFoundError:
        # FFmpegê°€ ì„¤ì¹˜ë˜ì§€ ì•ŠìŒ
        async with self._state_lock:
            self.state = FFmpegState.FAILED

        if self.on_error_callback:
            await self.on_error_callback("ffmpeg_not_found")

        return False
```

#### 4.5.3 ë°ì´í„° ì½ê¸°/ì“°ê¸°

```python
async def write_data(self, data: bytes):
    """FFmpeg stdinìœ¼ë¡œ ì˜¤ë””ì˜¤ ë°ì´í„° ì“°ê¸°"""
    async with self._state_lock:
        if self.state != FFmpegState.RUNNING:
            return False

    try:
        self.process.stdin.write(data)
        await self.process.stdin.drain()
        return True
    except Exception as e:
        logger.error(f"FFmpeg ì“°ê¸° ì˜¤ë¥˜: {e}")
        return False

async def read_data(self, size: int):
    """FFmpeg stdoutì—ì„œ PCM ë°ì´í„° ì½ê¸°"""
    async with self._state_lock:
        if self.state != FFmpegState.RUNNING:
            return None

    try:
        data = await asyncio.wait_for(
            self.process.stdout.read(size),
            timeout=20.0
        )
        return data
    except asyncio.TimeoutError:
        logger.warning("FFmpeg ì½ê¸° íƒ€ì„ì•„ì›ƒ")
        return None
```

---

## 5. ë°ì´í„° íë¦„

### 5.1 ì „ì²´ ë°ì´í„° íë¦„ë„

```
[ë¸Œë¼ìš°ì € ì˜¤ë””ì˜¤ ìº¡ì²˜]
        â”‚
        â”œâ”€ MediaRecorder â†’ WebM/Opus (ì••ì¶•)
        â””â”€ AudioWorklet â†’ PCM s16le (ë¹„ì••ì¶•)
        â”‚
        â†“ WebSocket Binary
        â”‚
[AudioProcessor.process_audio()]
        â”‚
        â”œâ”€ is_pcm_input == True
        â”‚  â””â†’ self.pcm_buffer.extend(message)
        â”‚
        â””â”€ is_pcm_input == False
           â””â†’ FFmpegManager.write_data(message)
              â””â†’ FFmpeg: WebM/Opus â†’ PCM s16le
                 â””â†’ FFmpegManager.read_data()
        â”‚
        â†“ PCM ë°°ì—´ (np.float32)
        â”‚
[handle_pcm_data()]
        â”‚
        â”œâ”€ Silero VAD ì‹¤í–‰
        â”‚  â”œâ”€ confidence > 0.5 â†’ ìŒì„±
        â”‚  â””â”€ confidence < 0.35 â†’ ì¹¨ë¬µ
        â”‚
        â””â”€ ìŒì„± í™œë™ ì‹œ
           â””â†’ transcription_queue.put(pcm_chunk)
        â”‚
        â†“
[transcription_processor()]
        â”‚
        â”œâ”€ SimulStreaming ë°±ì—”ë“œ
        â”‚  â”‚
        â”‚  â”œâ”€ 1. Encoder (Faster-Whisper/MLX)
        â”‚  â”‚    PCM â†’ Mel-Spectrogram â†’ Features
        â”‚  â”‚    [80, 3000] â†’ [1280, 1500]
        â”‚  â”‚
        â”‚  â”œâ”€ 2. AlignAtt Decoder
        â”‚  â”‚    â”œâ”€ Decoder State (tokens, kv_cache)
        â”‚  â”‚    â”œâ”€ Forward pass
        â”‚  â”‚    â”œâ”€ Attention Alignment ê³„ì‚°
        â”‚  â”‚    â”œâ”€ should_fire() ì²´í¬
        â”‚  â”‚    â””â”€ Token ìƒì„± (Beam/Greedy)
        â”‚  â”‚
        â”‚  â””â”€ 3. ASRToken ì¶œë ¥
        â”‚       ASRToken(start=1.2, end=1.5, text="hello")
        â”‚
        â””â”€ LocalAgreement ë°±ì—”ë“œ
           â”‚
           â”œâ”€ 1. Whisper ASR ì‹¤í–‰
           â”‚    PCM â†’ Mel â†’ Encoder â†’ Decoder â†’ Tokens
           â”‚
           â”œâ”€ 2. Hypothesis Buffer
           â”‚    â”œâ”€ ìƒˆë¡œìš´ ê°€ì„¤ ì‚½ì…
           â”‚    â”œâ”€ ì´ì „ ê°€ì„¤ê³¼ ë¹„êµ
           â”‚    â””â”€ Longest Common Prefix í™•ì¸
           â”‚
           â””â”€ 3. ASRToken ì¶œë ¥ (í™•ì •ëœ ê²ƒë§Œ)
        â”‚
        â†“ ASRToken ìŠ¤íŠ¸ë¦¼
        â”‚
[state.tokens ì—…ë°ì´íŠ¸]
        â”‚
        â””â†’ translation_queue.put(token)  (ë²ˆì—­ í™œì„±í™” ì‹œ)
        â”‚
        â†“
[diarization_processor()] (ì„ íƒ)
        â”‚
        â”œâ”€ SortformerDiarization
        â”‚  â”œâ”€ Mel ì¶”ì¶œ (10 í”„ë ˆì„ ì²­í¬)
        â”‚  â”œâ”€ Model Forward (speaker embeddings)
        â”‚  â”œâ”€ Speaker Clustering (ì˜¨ë¼ì¸)
        â”‚  â””â”€ SpeakerSegment ì¶œë ¥
        â”‚
        â””â”€ DiartDiarization
           â”œâ”€ RxPy Observable
           â”œâ”€ Segmentation + Embedding
           â””â”€ SpeakerSegment ì¶œë ¥
        â”‚
        â†“ SpeakerSegment ìŠ¤íŠ¸ë¦¼
        â”‚
[state.new_diarization ì—…ë°ì´íŠ¸]
        â”‚
        â†“
[translation_processor()] (ì„ íƒ)
        â”‚
        â”œâ”€ NLLW ëª¨ë¸ ë¡œë“œ
        â”œâ”€ Source Language: ê°ì§€ëœ ì–¸ì–´
        â”œâ”€ Target Language: ì„¤ì •ëœ ì–¸ì–´
        â””â”€ Translation ì¶œë ¥
        â”‚
        â†“ Translation ìŠ¤íŠ¸ë¦¼
        â”‚
[TokensAlignment.update()]
        â”‚
        â”œâ”€ state.new_tokens â†’ all_tokens
        â”œâ”€ state.new_diarization â†’ all_diarization_segments
        â””â”€ state.new_translation â†’ all_translation_segments
        â”‚
        â†“
[TokensAlignment.get_lines()]
        â”‚
        â”œâ”€ 1. Punctuation ê¸°ë°˜ ì„¸ê·¸ë¨¼íŠ¸ ìƒì„±
        â”‚    "Hello world." â†’ Segment
        â”‚
        â”œâ”€ 2. Speaker ë§¤í•‘
        â”‚    for token in segment:
        â”‚        find_overlapping_speaker_segment()
        â”‚        token.speaker = segment.speaker
        â”‚
        â”œâ”€ 3. Translation ë³‘í•©
        â”‚    for translation in translations:
        â”‚        if translation.is_within(segment):
        â”‚            segment.translation += translation.text
        â”‚
        â””â”€ 4. Segment ë¦¬ìŠ¤íŠ¸ ë°˜í™˜
           Segment(start, end, text, speaker, translation)
        â”‚
        â†“
[results_formatter()]
        â”‚
        â”œâ”€ FrontData ìƒì„±
        â”‚    {
        â”‚      status: "active_transcription",
        â”‚      lines: [Segment, ...],
        â”‚      buffer_transcription: "ongoing...",
        â”‚      buffer_diarization: "Speaker 1...",
        â”‚      buffer_translation: "ë²ˆì—­ ì¤‘...",
        â”‚    }
        â”‚
        â””â”€ JSON ë³€í™˜
           FrontData.to_dict()
        â”‚
        â†“ JSON ì‘ë‹µ
        â”‚
[WebSocket.send_json()]
        â”‚
        â†“
[ë¸Œë¼ìš°ì € UI ì—…ë°ì´íŠ¸]
        â”‚
        â””â”€ live_transcription.js
           â”œâ”€ onMessage(json)
           â”œâ”€ updateTranscript(lines)
           â””â”€ renderSegments(lines)
```

### 5.2 íƒ€ì´ë° ì •ë³´ íë¦„

```
[Audio Timeline]
0.0s â”€â”€â”€â”€â”€â”€â”€â”€ 1.0s â”€â”€â”€â”€â”€â”€â”€â”€ 2.0s â”€â”€â”€â”€â”€â”€â”€â”€ 3.0s â”€â”€â”€â”€â”€â”€â”€â”€ 4.0s
  â”‚            â”‚            â”‚            â”‚            â”‚
  â”‚ PCM Chunk  â”‚ PCM Chunk  â”‚ PCM Chunk  â”‚ PCM Chunk  â”‚
  â”‚ [0-1s]     â”‚ [1-2s]     â”‚ [2-3s]     â”‚ [3-4s]     â”‚
  â”‚            â”‚            â”‚            â”‚            â”‚
  â–¼            â–¼            â–¼            â–¼            â–¼
[ASR Processing]
  â”‚            â”‚            â”‚            â”‚            â”‚
  â”‚ Token 1    â”‚ Token 2    â”‚ Token 3    â”‚ Token 4    â”‚
  â”‚ "Hello"    â”‚ "world"    â”‚ "how"      â”‚ "are"      â”‚
  â”‚ [0.2-0.7s] â”‚ [0.8-1.3s] â”‚ [2.1-2.5s] â”‚ [2.6-3.1s] â”‚
  â”‚            â”‚            â”‚            â”‚            â”‚

[Diarization Processing]
  â”‚                                                    â”‚
  â”‚ Speaker Segment 1          Speaker Segment 2      â”‚
  â”‚ Speaker 0 [0.0-2.0s]       Speaker 1 [2.0-4.0s]   â”‚
  â”‚                                                    â”‚

[Alignment]
  â”‚                                                    â”‚
  â”‚ Segment 1                  Segment 2              â”‚
  â”‚ "Hello world"              "how are"              â”‚
  â”‚ Speaker 0 [0.2-1.3s]       Speaker 1 [2.1-3.1s]   â”‚
  â”‚                                                    â”‚

[Output Timeline]
  â”‚
  â”œâ”€ t=0.7s: {"text": "Hello", "speaker": 0, "is_final": false}
  â”œâ”€ t=1.3s: {"text": "Hello world", "speaker": 0, "is_final": true}
  â”œâ”€ t=2.5s: {"text": "how", "speaker": 1, "is_final": false}
  â””â”€ t=3.1s: {"text": "how are", "speaker": 1, "is_final": true}
```

---

## 6. ì£¼ìš” ì•Œê³ ë¦¬ì¦˜

### 6.1 AlignAtt (Attention Alignment)

#### 6.1.1 ì•Œê³ ë¦¬ì¦˜ ê°œìš”

**ëª©í‘œ**: Cross-Attention ê°€ì¤‘ì¹˜ë¥¼ ë¶„ì„í•˜ì—¬ ë‹¨ì–´ ê²½ê³„ë¥¼ ì‹¤ì‹œê°„ìœ¼ë¡œ ì˜ˆì¸¡

**ì›ë¦¬**:
- Encoder-Decoder ì•„í‚¤í…ì²˜ì—ì„œ DecoderëŠ” Encoder ì¶œë ¥ì— ëŒ€í•´ Cross-Attention ìˆ˜í–‰
- Attention ê°€ì¤‘ì¹˜ê°€ í˜„ì¬ ë‹¨ì–´ì—ì„œ ë‹¤ìŒ ë‹¨ì–´ë¡œ "ì´ë™"í•˜ëŠ” ì‹œì  ê°ì§€
- ì´ë™ ì‹œì  = ë‹¨ì–´ ê²½ê³„ = ë°œí™”(Fire) ì§€ì 

#### 6.1.2 ìˆ˜ì‹

```
1. Cross-Attention ê³„ì‚°:
   Attention(Q, K, V) = softmax(QK^T / âˆšd_k) V

   ì—¬ê¸°ì„œ:
   - Q: Decoderì˜ Query (í˜„ì¬ í† í°)
   - K, V: Encoderì˜ Key, Value (ì˜¤ë””ì˜¤ í”„ë ˆì„)
   - Attention ê°€ì¤‘ì¹˜: Î± = softmax(QK^T / âˆšd_k)
     shape: [num_tokens, num_frames]

2. Alignment Score ê³„ì‚°:
   alignment_heads = [(layer_i, head_j), ...]  # ì‚¬ì „ ì¶”ì¶œëœ í—¤ë“œ

   Î±_avg = mean([Î±[layer_i, head_j] for (layer_i, head_j) in alignment_heads])

3. Fire ì¡°ê±´:
   last_token_attention = Î±_avg[-1]  # ë§ˆì§€ë§‰ í† í°ì˜ attention

   recent_attention = sum(last_token_attention[-threshold:])

   if recent_attention > fire_threshold:
       fire = True  # ë‹¨ì–´ ì™„ì„±, ë‹¤ìŒ í† í° ìƒì„±
```

#### 6.1.3 ì˜ˆì œ

```
ì˜¤ë””ì˜¤: "Hello world"
Frames: [f0, f1, f2, ..., f99]  (100 í”„ë ˆì„ = 2ì´ˆ)

Token 1: "<SOT>"
  Attention: [0.01, 0.01, ..., 0.01]  # ê· ë“± ë¶„í¬

Token 2: "Hello"
  Decoding Step 1:
    Attention: [0.8, 0.2, ..., 0.0]  # f0-f10ì— ì§‘ì¤‘
    recent_attention (last 25 frames) = 0.05
    â†’ fire = False (ê³„ì† ëŒ€ê¸°)

  Decoding Step 2 (0.1ì´ˆ í›„):
    Attention: [0.6, 0.4, ..., 0.0]  # f10-f20ì— ì§‘ì¤‘
    recent_attention = 0.1
    â†’ fire = False

  Decoding Step 3 (0.2ì´ˆ í›„):
    Attention: [0.2, 0.3, 0.5, ..., 0.0]  # f20-f30ìœ¼ë¡œ ì´ë™
    recent_attention = 0.3
    â†’ fire = True âœ“

    â†’ Output: ASRToken("Hello", start=0.0, end=0.6)

Token 3: "world"
  (ë°˜ë³µ...)
```

---

### 6.2 LocalAgreement (Hypothesis Buffer)

#### 6.2.1 ì•Œê³ ë¦¬ì¦˜ ê°œìš”

**ëª©í‘œ**: ì—¬ëŸ¬ ê°€ì„¤ì„ ë¹„êµí•˜ì—¬ ì•ˆì •ì ì¸(í™•ì •ëœ) í† í°ë§Œ ì¶œë ¥

**ì›ë¦¬**:
- ê° ì˜¤ë””ì˜¤ ì²­í¬ë§ˆë‹¤ ë…ë¦½ì ìœ¼ë¡œ ASR ì‹¤í–‰ â†’ ê°€ì„¤ ìƒì„±
- ì´ì „ ê°€ì„¤ê³¼ í˜„ì¬ ê°€ì„¤ì˜ Longest Common Prefix (LCP) ì°¾ê¸°
- LCP = í™•ì •ëœ í…ìŠ¤íŠ¸ â†’ ì¶œë ¥
- ë‚˜ë¨¸ì§€ = ë¶ˆí™•ì‹¤ â†’ ë‹¤ìŒ ì²­í¬ì—ì„œ ì¬ê²€ì¦

#### 6.2.2 ì˜ì‚¬ ì½”ë“œ

```python
class HypothesisBuffer:
    def __init__(self):
        self.committed_tokens = []  # í™•ì •ëœ í† í°
        self.buffer_tokens = []     # ëŒ€ê¸° ì¤‘ì¸ í† í°

    def insert(self, new_hypothesis, offset):
        """
        ìƒˆë¡œìš´ ê°€ì„¤ ì‚½ì…

        Args:
            new_hypothesis: ["Hello", "world", "how"]
            offset: ì˜¤ë””ì˜¤ ì‹œì‘ ì‹œê°„
        """
        # 1. ì´ì „ ë²„í¼ì™€ ìƒˆ ê°€ì„¤ ë¹„êµ
        lcp_len = self.find_longest_common_prefix(
            self.buffer_tokens,
            new_hypothesis
        )

        # 2. LCP ë¶€ë¶„ì„ í™•ì •
        for i in range(lcp_len):
            if i < len(self.buffer_tokens):
                self.committed_tokens.append(self.buffer_tokens[i])

        # 3. ë‚˜ë¨¸ì§€ë¥¼ ìƒˆ ë²„í¼ë¡œ ì„¤ì •
        self.buffer_tokens = new_hypothesis[lcp_len:]

    def find_longest_common_prefix(self, tokens1, tokens2):
        """
        ë‘ í† í° ë¦¬ìŠ¤íŠ¸ì˜ LCP ê¸¸ì´ ë°˜í™˜

        Example:
            tokens1 = ["Hello", "world", "how"]
            tokens2 = ["Hello", "world", "are"]
            â†’ LCP = 2 (["Hello", "world"])
        """
        lcp_len = 0
        for t1, t2 in zip(tokens1, tokens2):
            if t1 == t2:
                lcp_len += 1
            else:
                break
        return lcp_len

    def flush(self):
        """í™•ì •ëœ í† í° ë°˜í™˜"""
        result = self.committed_tokens.copy()
        self.committed_tokens.clear()
        return result
```

#### 6.2.3 ì˜ˆì œ

```
Chunk 1 (0-1ì´ˆ):
  ASR â†’ ["Hello"]
  Buffer: ["Hello"]
  Committed: []
  Output: []

Chunk 2 (0-2ì´ˆ):
  ASR â†’ ["Hello", "world"]
  LCP(["Hello"], ["Hello", "world"]) = 1
  Committed: ["Hello"]
  Buffer: ["world"]
  Output: ["Hello"]

Chunk 3 (0-3ì´ˆ):
  ASR â†’ ["Hello", "world", "how"]
  LCP(["world"], ["world", "how"]) = 1
  Committed: ["world"]
  Buffer: ["how"]
  Output: ["world"]

Chunk 4 (0-4ì´ˆ):
  ASR â†’ ["Hello", "world", "how", "are"]
  LCP(["how"], ["how", "are"]) = 1
  Committed: ["how"]
  Buffer: ["are"]
  Output: ["how"]

Chunk 5 (0-5ì´ˆ):
  ASR â†’ ["Hello", "world", "how", "are", "you"]
  LCP(["are"], ["are", "you"]) = 1
  Committed: ["are"]
  Buffer: ["you"]
  Output: ["are"]
```

---

### 6.3 CIF (Continuous Integrate-and-Fire)

#### 6.3.1 ì•Œê³ ë¦¬ì¦˜ ê°œìš”

**ëª©í‘œ**: Encoder ì¶œë ¥ì—ì„œ ì§ì ‘ ë‹¨ì–´ ê²½ê³„ë¥¼ ì˜ˆì¸¡ (Alignment Heads ë¶ˆí•„ìš”)

**ì›ë¦¬**:
- Encoder ê° í”„ë ˆì„ì— "ë°œí™” í™•ë¥ " ë¶€ì—¬
- í™•ë¥ ì˜ ëˆ„ì  í•©ì´ ì •ìˆ˜ë¥¼ ë„˜ì„ ë•Œ = ë‹¨ì–´ ê²½ê³„

#### 6.3.2 ìˆ˜ì‹

```
1. CIF ë ˆì´ì–´:
   h_t = Encoder(audio)[t]  # të²ˆì§¸ í”„ë ˆì„ì˜ hidden state

   Î±_t = sigmoid(W_cif @ h_t + b_cif)  # ë°œí™” í™•ë¥  [0, 1]

2. ëˆ„ì  í•©:
   C_t = Î£ Î±_i  (i=0 to t)

3. Fire ì¡°ê±´:
   if floor(C_t) > floor(C_{t-1}):
       fire = True
       emit_word()
```

#### 6.3.3 ì˜ˆì œ

```
í”„ë ˆì„:  f0    f1    f2    f3    f4    f5    f6
Î±:      0.1   0.2   0.7   0.3   0.4   0.6   0.2
C:      0.1   0.3   1.0   1.3   1.7   2.3   2.5
                     â†‘              â†‘
                   Fire 1        Fire 2

Output:
  - t=2: ë‹¨ì–´ 1 ì™„ì„± ("Hello")
  - t=5: ë‹¨ì–´ 2 ì™„ì„± ("world")
```

---

### 6.4 Beam Search

#### 6.4.1 ì•Œê³ ë¦¬ì¦˜ ê°œìš”

**ëª©í‘œ**: ì—¬ëŸ¬ ê°€ëŠ¥í•œ í† í° ì‹œí€€ìŠ¤ë¥¼ ë™ì‹œì— íƒìƒ‰í•˜ì—¬ ìµœì  ê²½ë¡œ ì°¾ê¸°

**ì›ë¦¬**:
- ê° ë‹¨ê³„ì—ì„œ ìƒìœ„ Kê°œì˜ ê°€ì„¤(Beam) ìœ ì§€
- ê° ê°€ì„¤ì—ì„œ ë‹¤ìŒ í† í° ìƒì„±
- K Ã— Vê°œì˜ í›„ë³´ ì¤‘ ìƒìœ„ Kê°œ ì„ íƒ (V = vocab size)

#### 6.4.2 ì˜ì‚¬ ì½”ë“œ

```python
def beam_search(model, encoder_output, beam_size=5, max_length=448):
    # ì´ˆê¸° ê°€ì„¤: [<SOT>]
    beams = [Beam(tokens=[SOT_TOKEN], score=0.0)]

    for step in range(max_length):
        all_candidates = []

        for beam in beams:
            # ë””ì½”ë” ì‹¤í–‰
            logits = model.decoder(beam.tokens, encoder_output)
            log_probs = F.log_softmax(logits[-1], dim=-1)

            # ìƒìœ„ Kê°œ í† í° ì„ íƒ
            top_k_probs, top_k_tokens = torch.topk(log_probs, beam_size)

            for prob, token in zip(top_k_probs, top_k_tokens):
                new_beam = Beam(
                    tokens=beam.tokens + [token],
                    score=beam.score + prob
                )
                all_candidates.append(new_beam)

        # ìƒìœ„ Kê°œ beam ì„ íƒ
        all_candidates.sort(key=lambda b: b.score / len(b.tokens), reverse=True)
        beams = all_candidates[:beam_size]

        # ì¢…ë£Œ ì¡°ê±´
        if all(beam.tokens[-1] == EOT_TOKEN for beam in beams):
            break

    # ìµœê³  ì ìˆ˜ beam ë°˜í™˜
    return beams[0]
```

#### 6.4.3 ì˜ˆì œ

```
Beam Size = 3

Step 0:
  Beam 1: [<SOT>], score=0

Step 1:
  Beam 1 â†’ ["Hello"] (score=-0.5)
  Beam 1 â†’ ["Hi"] (score=-0.8)
  Beam 1 â†’ ["Hey"] (score=-1.2)

  Selected: [["Hello"], ["Hi"], ["Hey"]]

Step 2:
  ["Hello"] â†’ ["Hello", "world"] (score=-1.0)
  ["Hello"] â†’ ["Hello", "there"] (score=-1.3)
  ["Hi"] â†’ ["Hi", "there"] (score=-1.5)
  ["Hi"] â†’ ["Hi", "everyone"] (score=-1.8)
  ["Hey"] â†’ ["Hey", "there"] (score=-2.0)

  Selected: [
    ["Hello", "world"],
    ["Hello", "there"],
    ["Hi", "there"]
  ]

... (ë°˜ë³µ)

Final:
  Best Beam: ["Hello", "world", "how", "are", "you"]
```

---

## 7. ì„¤ì¹˜ ë° ì‚¬ìš©ë²•

### 7.1 ì‹œìŠ¤í…œ ìš”êµ¬ì‚¬í•­

#### 7.1.1 í•˜ë“œì›¨ì–´

| ì»´í¬ë„ŒíŠ¸ | ìµœì†Œ ì‚¬ì–‘ | ê¶Œì¥ ì‚¬ì–‘ |
|---------|----------|----------|
| **CPU** | 2 ì½”ì–´ | 4+ ì½”ì–´ |
| **RAM** | 4GB | 8GB+ |
| **GPU** | ì—†ìŒ (CPU ëª¨ë“œ) | NVIDIA GPU (4GB+ VRAM) |
| **ë””ìŠ¤í¬** | 2GB | 10GB+ (ì—¬ëŸ¬ ëª¨ë¸) |

#### 7.1.2 ì†Œí”„íŠ¸ì›¨ì–´

- **Python**: 3.9 - 3.15
- **OS**: Windows, macOS, Linux
- **FFmpeg**: ìµœì‹  ë²„ì „ (ì„ íƒ, `--pcm-input` ì‚¬ìš© ì‹œ ë¶ˆí•„ìš”)

### 7.2 ì„¤ì¹˜

#### 7.2.1 ê¸°ë³¸ ì„¤ì¹˜

```bash
# PyPIì—ì„œ ì„¤ì¹˜
pip install whisperlivekit

# ë˜ëŠ” ìµœì‹  ë²„ì „ (GitHub)
git clone https://github.com/QuentinFuxa/WhisperLiveKit.git
cd WhisperLiveKit
pip install -e .
```

#### 7.2.2 ì„ íƒì  ì˜ì¡´ì„±

```bash
# Faster-Whisper (Windows/Linux ìµœì í™”)
pip install faster-whisper

# MLX-Whisper (Apple Silicon ìµœì í™”)
pip install mlx-whisper

# ë²ˆì—­ (200ê°œ ì–¸ì–´)
pip install nllw

# í™”ì ì‹ë³„ (Sortformer)
pip install git+https://github.com/NVIDIA/NeMo.git@main#egg=nemo_toolkit[asr]

# í™”ì ì‹ë³„ (Diart, ë¹„ê¶Œì¥)
pip install diart
```

### 7.3 ê¸°ë³¸ ì‚¬ìš©ë²•

#### 7.3.1 ë¹ ë¥¸ ì‹œì‘

```bash
# 1. ì„œë²„ ì‹œì‘ (base ëª¨ë¸, ì˜ì–´)
wlk --model base --language en

# 2. ë¸Œë¼ìš°ì €ì—ì„œ http://localhost:8000 ì ‘ì†

# 3. ë§ˆì´í¬ ê¶Œí•œ í—ˆìš© â†’ ë§í•˜ê¸° ì‹œì‘
```

#### 7.3.2 ê³ ê¸‰ ì‚¬ìš©ë²•

```bash
# Large ëª¨ë¸ + í”„ë‘ìŠ¤ì–´ â†’ ë´ë§ˆí¬ì–´ ë²ˆì—­
wlk --model large-v3 --language fr --target-language da

# í™”ì ì‹ë³„ í™œì„±í™”
wlk --model medium --diarization --language en

# ì™¸ë¶€ ì ‘ì† í—ˆìš© (80 í¬íŠ¸)
wlk --host 0.0.0.0 --port 80 --model small

# HTTPS í™œì„±í™”
wlk --ssl-certfile cert.pem --ssl-keyfile key.pem

# PCM ëª¨ë“œ (FFmpeg ë¶ˆí•„ìš”)
wlk --pcm-input --model base

# Apple Silicon ìµœì í™”
wlk --backend mlx-whisper --model medium --language en
```

### 7.4 Python API ì‚¬ìš©

```python
import asyncio
from contextlib import asynccontextmanager
from fastapi import FastAPI, WebSocket
from whisperlivekit import AudioProcessor, TranscriptionEngine

# ì „ì—­ ì—”ì§„ (ì‹±ê¸€í†¤)
transcription_engine = None

@asynccontextmanager
async def lifespan(app: FastAPI):
    global transcription_engine
    # ì—”ì§„ ì´ˆê¸°í™” (ì„œë²„ ì‹œì‘ ì‹œ 1íšŒ)
    transcription_engine = TranscriptionEngine(
        model_size="medium",
        diarization=True,
        lan="en"
    )
    yield

app = FastAPI(lifespan=lifespan)

async def handle_websocket_results(websocket, results_generator):
    """ê²°ê³¼ ìŠ¤íŠ¸ë¦¼ ì²˜ë¦¬"""
    async for response in results_generator:
        await websocket.send_json(response.to_dict())
    await websocket.send_json({"type": "ready_to_stop"})

@app.websocket("/asr")
async def websocket_endpoint(websocket: WebSocket):
    global transcription_engine

    # ì—°ê²°ë³„ AudioProcessor ìƒì„±
    audio_processor = AudioProcessor(
        transcription_engine=transcription_engine
    )

    # ì‘ì—… ì‹œì‘
    results_generator = await audio_processor.create_tasks()
    results_task = asyncio.create_task(
        handle_websocket_results(websocket, results_generator)
    )

    await websocket.accept()

    try:
        while True:
            message = await websocket.receive_bytes()
            await audio_processor.process_audio(message)
    except Exception as e:
        print(f"ì˜¤ë¥˜: {e}")
    finally:
        await audio_processor.cleanup()
```

---

## 8. ì„±ëŠ¥ ìµœì í™”

### 8.1 ì§€ì—°ì‹œê°„ ìµœì í™”

#### 8.1.1 ë°±ì—”ë“œ ì„ íƒ

| ë°±ì—”ë“œ | ì§€ì—°ì‹œê°„ | ì •í™•ë„ | ê¶Œì¥ í™˜ê²½ |
|--------|---------|-------|----------|
| **MLX-Whisper** | 50-150ms | ë†’ìŒ | Apple Silicon (M1/M2/M3) |
| **Faster-Whisper** | 100-300ms | ë†’ìŒ | NVIDIA GPU, CPU |
| **PyTorch** | 300-800ms | ìµœê³  | NVIDIA GPU |
| **OpenAI API** | 1000-3000ms | ìµœê³  | í´ë¼ìš°ë“œ |

#### 8.1.2 ì •ì±… ì„ íƒ

```bash
# ìµœì € ì§€ì—°ì‹œê°„ (300-800ms)
wlk --backend-policy simulstreaming --frame-threshold 15

# ê· í˜• (1-2ì´ˆ)
wlk --backend-policy simulstreaming --frame-threshold 25

# ìµœê³  ì •í™•ë„ (2-3ì´ˆ)
wlk --backend-policy localagreement --buffer-trimming sentence
```

#### 8.1.3 ëª¨ë¸ í¬ê¸° ì„ íƒ

| ëª¨ë¸ | íŒŒë¼ë¯¸í„° | ë©”ëª¨ë¦¬ | ì¶”ë¡  ì†ë„ | ì •í™•ë„ |
|------|---------|-------|-----------|--------|
| **tiny** | 39M | 400MB | 32x | ë‚®ìŒ |
| **base** | 74M | 600MB | 16x | ì¤‘ê°„ |
| **small** | 244M | 1.5GB | 6x | ë†’ìŒ |
| **medium** | 769M | 3GB | 2x | ë§¤ìš° ë†’ìŒ |
| **large-v3** | 1550M | 6GB | 1x | ìµœê³  |

**ê¶Œì¥**:
- ì‹¤ì‹œê°„: `base` ë˜ëŠ” `small`
- ì˜¤í”„ë¼ì¸ ì „ì‚¬: `large-v3`

### 8.2 ë©”ëª¨ë¦¬ ìµœì í™”

#### 8.2.1 KV-Cache ì •ë¦¬

```python
# core.py ë˜ëŠ” audio_processor.py ìˆ˜ì •
class AudioProcessor:
    async def cleanup(self):
        # ëª…ì‹œì  ìºì‹œ ì •ë¦¬
        if hasattr(self.transcription, 'decoder_state'):
            self.transcription.decoder_state.clean_cache()

        # GPU ë©”ëª¨ë¦¬ í•´ì œ
        import torch
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
```

#### 8.2.2 ëª¨ë¸ ì–‘ìí™”

```bash
# Faster-WhisperëŠ” ìë™ìœ¼ë¡œ INT8 ì–‘ìí™” ì‚¬ìš©
wlk --backend faster-whisper --model large-v3
# â†’ 6GB VRAM â†’ 2GB VRAM
```

### 8.3 ë‹¤ì¤‘ ì‚¬ìš©ì ìµœì í™”

#### 8.3.1 ëª¨ë¸ ì ê¸ˆ ë¹„í™œì„±í™” (ë‹¨ì¼ ì‚¬ìš©ì)

```bash
# í™˜ê²½ ë³€ìˆ˜ ì„¤ì •
export WHISPERLIVEKIT_MODEL_LOCK=0

# ì„œë²„ ì‹œì‘
wlk --model base
```

#### 8.3.2 ëª¨ë¸ ì ê¸ˆ í™œì„±í™” (ë‹¤ì¤‘ ì‚¬ìš©ì)

```bash
export WHISPERLIVEKIT_MODEL_LOCK=1
export WHISPERLIVEKIT_LOCK_TIMEOUT=30  # 30ì´ˆ

# Gunicornìœ¼ë¡œ ë‹¤ì¤‘ ì›Œì»¤
gunicorn -k uvicorn.workers.UvicornWorker -w 4 \
  whisperlivekit.basic_server:app
```

---

## 9. ë°°í¬ ê°€ì´ë“œ

### 9.1 Docker ë°°í¬

#### 9.1.1 GPU ì§€ì›

```bash
# 1. ì´ë¯¸ì§€ ë¹Œë“œ
docker build -t wlk .

# 2. ì»¨í…Œì´ë„ˆ ì‹¤í–‰
docker run --gpus all -p 8000:8000 --name wlk wlk

# 3. ì»¤ìŠ¤í…€ ì„¤ì •
docker run --gpus all -p 8000:8000 wlk \
  --model large-v3 \
  --language fr \
  --diarization
```

#### 9.1.2 CPU ì „ìš©

```bash
# 1. CPU Dockerfile ì‚¬ìš©
docker build -f Dockerfile.cpu -t wlk-cpu .

# 2. ì‹¤í–‰
docker run -p 8000:8000 wlk-cpu --model base
```

### 9.2 Nginx ë¦¬ë²„ìŠ¤ í”„ë¡ì‹œ

```nginx
# /etc/nginx/sites-available/whisperlivekit
server {
    listen 80;
    server_name your-domain.com;

    location / {
        proxy_pass http://localhost:8000;
        proxy_set_header Upgrade $http_upgrade;
        proxy_set_header Connection "upgrade";
        proxy_set_header Host $host;
        proxy_set_header X-Real-IP $remote_addr;
        proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
        proxy_set_header X-Forwarded-Proto $scheme;
    }
}
```

### 9.3 HTTPS ì„¤ì •

#### 9.3.1 Let's Encrypt

```bash
# 1. Certbot ì„¤ì¹˜
sudo apt install certbot python3-certbot-nginx

# 2. ì¸ì¦ì„œ ë°œê¸‰
sudo certbot --nginx -d your-domain.com

# 3. WhisperLiveKitì— SSL ì „ë‹¬
wlk --ssl-certfile /etc/letsencrypt/live/your-domain.com/fullchain.pem \
    --ssl-keyfile /etc/letsencrypt/live/your-domain.com/privkey.pem
```

#### 9.3.2 ìì²´ ì„œëª… ì¸ì¦ì„œ

```bash
# 1. ì¸ì¦ì„œ ìƒì„±
openssl req -x509 -newkey rsa:4096 -keyout key.pem -out cert.pem -days 365 -nodes

# 2. ì„œë²„ ì‹œì‘
wlk --ssl-certfile cert.pem --ssl-keyfile key.pem
```

### 9.4 í”„ë¡œë•ì…˜ ë°°í¬ ì²´í¬ë¦¬ìŠ¤íŠ¸

- [ ] FFmpeg ì„¤ì¹˜ í™•ì¸ (`ffmpeg -version`)
- [ ] GPU ë“œë¼ì´ë²„ ì„¤ì¹˜ (CUDA, cuDNN)
- [ ] ë°©í™”ë²½ í¬íŠ¸ ê°œë°© (8000 ë˜ëŠ” ì»¤ìŠ¤í…€)
- [ ] í™˜ê²½ ë³€ìˆ˜ ì„¤ì • (WHISPERLIVEKIT_MODEL_LOCK)
- [ ] ë¡œê·¸ ë””ë ‰í† ë¦¬ ìƒì„± (`/var/log/whisperlivekit/`)
- [ ] Systemd ì„œë¹„ìŠ¤ ë“±ë¡
- [ ] ëª¨ë‹ˆí„°ë§ ì„¤ì • (Prometheus, Grafana)
- [ ] ë°±ì—… ì „ëµ ìˆ˜ë¦½

---

## 10. ë¬¸ì œ í•´ê²°

### 10.1 ì¼ë°˜ì ì¸ ì˜¤ë¥˜

#### 10.1.1 FFmpeg ê´€ë ¨

**ì˜¤ë¥˜**: `FFmpeg is not installed`

**í•´ê²°**:
```bash
# Ubuntu/Debian
sudo apt update && sudo apt install ffmpeg

# macOS
brew install ffmpeg

# Windows
# https://ffmpeg.org/download.htmlì—ì„œ ë‹¤ìš´ë¡œë“œ
# PATHì— bin í´ë” ì¶”ê°€

# ë˜ëŠ” PCM ëª¨ë“œ ì‚¬ìš©
wlk --pcm-input
```

#### 10.1.2 GPU ë©”ëª¨ë¦¬ ë¶€ì¡±

**ì˜¤ë¥˜**: `CUDA out of memory`

**í•´ê²°**:
```bash
# 1. ë” ì‘ì€ ëª¨ë¸ ì‚¬ìš©
wlk --model small  # large-v3 ëŒ€ì‹ 

# 2. Fast Encoder ë¹„í™œì„±í™”
wlk --disable-fast-encoder

# 3. Batch Size ê°ì†Œ (ì½”ë“œ ìˆ˜ì • í•„ìš”)

# 4. Faster-Whisper ì‚¬ìš© (ì–‘ìí™”)
wlk --backend faster-whisper --model large-v3
```

#### 10.1.3 ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨

**ì˜¤ë¥˜**: `Failed to download model`

**í•´ê²°**:
```bash
# 1. HuggingFace í† í° ì„¤ì • (ê²Œì´íŠ¸ëœ ëª¨ë¸)
huggingface-cli login

# 2. ìˆ˜ë™ ë‹¤ìš´ë¡œë“œ
python scripts/convert_hf_whisper.py \
  --repo openai/whisper-large-v3 \
  --output ./models/large-v3

# 3. ë¡œì»¬ ëª¨ë¸ ì‚¬ìš©
wlk --model-path ./models/large-v3
```

### 10.2 ì„±ëŠ¥ ë¬¸ì œ

#### 10.2.1 ë†’ì€ ì§€ì—°ì‹œê°„

**ì¦ìƒ**: 3ì´ˆ ì´ìƒ ì§€ì—°

**í•´ê²°**:
```bash
# 1. ë°±ì—”ë“œ ë³€ê²½
wlk --backend mlx-whisper  # Apple Silicon
wlk --backend faster-whisper  # Others

# 2. ì •ì±… ë³€ê²½
wlk --backend-policy simulstreaming

# 3. Frame Threshold ë‚®ì¶”ê¸°
wlk --frame-threshold 15

# 4. ì‘ì€ ëª¨ë¸ ì‚¬ìš©
wlk --model small
```

#### 10.2.2 ë‚®ì€ ì •í™•ë„

**ì¦ìƒ**: ì˜ëª»ëœ ì „ì‚¬

**í•´ê²°**:
```bash
# 1. í° ëª¨ë¸ ì‚¬ìš©
wlk --model large-v3

# 2. ì–¸ì–´ ëª…ì‹œ
wlk --language en  # auto ëŒ€ì‹ 

# 3. Frame Threshold ë†’ì´ê¸°
wlk --frame-threshold 35

# 4. ì´ˆê¸° í”„ë¡¬í”„íŠ¸ ì‚¬ìš©
wlk --init-prompt "Technical discussion about AI"
```

---

## 11. ê°œë°œì ê°€ì´ë“œ

### 11.1 í”„ë¡œì íŠ¸ êµ¬ì¡° ì´í•´

#### 11.1.1 ë ˆì´ì–´ ì•„í‚¤í…ì²˜

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         Presentation Layer              â”‚
â”‚  (basic_server.py, web_interface.py)    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                  â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         Application Layer               â”‚
â”‚  (audio_processor.py, tokens_alignment) â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                  â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚           Domain Layer                  â”‚
â”‚  (core.py, simul_whisper, whisper)      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                  â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚       Infrastructure Layer              â”‚
â”‚  (ffmpeg_manager, silero_vad_iterator)  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 11.2 ì»¤ìŠ¤í…€ ë°±ì—”ë“œ ì¶”ê°€

#### 11.2.1 ìƒˆ ASR ë°±ì—”ë“œ êµ¬í˜„

```python
# whisperlivekit/local_agreement/backends.py

class CustomASR:
    def __init__(self, model_size="base", lan="en"):
        # ëª¨ë¸ ë¡œë“œ
        self.model = load_custom_model(model_size)
        self.language = lan

    def transcribe(self, audio, prompt=None):
        """
        ì˜¤ë””ì˜¤ ì „ì‚¬

        Args:
            audio: numpy array [samples]
            prompt: ì„ íƒì  ì´ˆê¸° í”„ë¡¬í”„íŠ¸

        Returns:
            result: {'text': str, 'segments': [...]}
        """
        # ì „ì‚¬ ë¡œì§ êµ¬í˜„
        result = self.model(audio, language=self.language)
        return result

    def ts_words(self, segments, start_time):
        """
        íƒ€ì„ìŠ¤íƒ¬í”„ê°€ ìˆëŠ” ë‹¨ì–´ ì¶”ì¶œ

        Args:
            segments: transcribe() ë°˜í™˜ê°’
            start_time: ì˜¤ë””ì˜¤ ì‹œì‘ ì‹œê°„

        Returns:
            words: [ASRToken, ...]
        """
        words = []
        for segment in segments:
            for word in segment['words']:
                token = ASRToken(
                    start=start_time + word['start'],
                    end=start_time + word['end'],
                    text=word['word']
                )
                words.append(token)
        return words

# whisperlivekit/local_agreement/whisper_online.py

def backend_factory(backend="auto", **kwargs):
    if backend == "custom":
        return CustomASR(**kwargs)
    # ê¸°ì¡´ ë°±ì—”ë“œ...
```

### 11.3 í…ŒìŠ¤íŠ¸

```bash
# ë‹¨ìœ„ í…ŒìŠ¤íŠ¸ (ì˜ˆì •)
pytest tests/

# í†µí•© í…ŒìŠ¤íŠ¸
python -m whisperlivekit.basic_server &
curl -X POST http://localhost:8000/asr \
  -H "Content-Type: audio/wav" \
  --data-binary @test.wav
```

---

## 12. FAQ

### Q1. WhisperLiveKitê³¼ OpenAI Whisper APIì˜ ì°¨ì´ì ì€?

**A**:
- **WhisperLiveKit**: ìì²´ í˜¸ìŠ¤íŒ…, ì‹¤ì‹œê°„ ìŠ¤íŠ¸ë¦¬ë°, ë¬´ë£Œ, í”„ë¼ì´ë²„ì‹œ ë³´í˜¸
- **OpenAI API**: í´ë¼ìš°ë“œ, ë°°ì¹˜ ì²˜ë¦¬, ìœ ë£Œ, ì™¸ë¶€ ì „ì†¡

### Q2. ì§€ì›ë˜ëŠ” ì–¸ì–´ëŠ”?

**A**: 99ê°œ ì–¸ì–´ ì¸ì‹, 200ê°œ ì–¸ì–´ ë²ˆì—­ ì§€ì›. [ì „ì²´ ëª©ë¡](docs/supported_languages.md)

### Q3. ëª¨ë¸ì„ ì–´ë–»ê²Œ êµì²´í•˜ë‚˜ìš”?

**A**:
```bash
# HuggingFaceì—ì„œ ìë™ ë‹¤ìš´ë¡œë“œ
wlk --model large-v3

# ë¡œì»¬ ê²½ë¡œ
wlk --model-path /path/to/model

# HuggingFace ì €ì¥ì†Œ
wlk --model-path openai/whisper-large-v3-turbo
```

### Q4. í™”ì ì‹ë³„ ì •í™•ë„ë¥¼ ë†’ì´ë ¤ë©´?

**A**:
```bash
# Sortformer ì‚¬ìš© (SOTA 2025)
wlk --diarization --diarization-backend sortformer

# ë” ë‚˜ì€ ì„¸ê·¸ë¨¼íŠ¸ ëª¨ë¸ (Diart)
wlk --diarization --diarization-backend diart \
  --segmentation-model pyannote/segmentation-3.0
```

### Q5. ìƒìš© í”„ë¡œì íŠ¸ì— ì‚¬ìš©í•  ìˆ˜ ìˆë‚˜ìš”?

**A**: ë„¤, MIT/Apache 2.0 ë¼ì´ì„¼ìŠ¤ë¡œ ìƒì—…ì  ì‚¬ìš© ê°€ëŠ¥í•©ë‹ˆë‹¤.

---

## 13. ê¸°ì—¬ ë° ë¼ì´ì„¼ìŠ¤

### 13.1 ê¸°ì—¬ ë°©ë²•

1. Fork ì €ì¥ì†Œ
2. ìƒˆ ë¸Œëœì¹˜ ìƒì„± (`git checkout -b feature/amazing-feature`)
3. ë³€ê²½ì‚¬í•­ ì»¤ë°‹ (`git commit -m 'Add amazing feature'`)
4. ë¸Œëœì¹˜ í‘¸ì‹œ (`git push origin feature/amazing-feature`)
5. Pull Request ìƒì„±

### 13.2 ë¼ì´ì„¼ìŠ¤

- **ì½”ë“œ**: MIT / Apache 2.0
- **ëª¨ë¸**: OpenAI Whisper - MIT
- **ë¬¸ì„œ**: CC BY 4.0

---

## 14. ì°¸ê³  ìë£Œ

### 14.1 ê³µì‹ ë¬¸ì„œ

- [GitHub ì €ì¥ì†Œ](https://github.com/QuentinFuxa/WhisperLiveKit)
- [API ë¬¸ì„œ](docs/API.md)
- [ê¸°ìˆ  í†µí•© ê°€ì´ë“œ](docs/technical_integration.md)

### 14.2 ê´€ë ¨ ë…¼ë¬¸

- [SimulWhisper](https://arxiv.org/pdf/2406.10052)
- [SimulStreaming](https://arxiv.org/abs/2506.17077)
- [AlignAtt](https://arxiv.org/pdf/2305.11408)
- [NLLB](https://arxiv.org/abs/2207.04672)
- [Streaming Sortformer](https://arxiv.org/abs/2507.18446)

### 14.3 ì»¤ë®¤ë‹ˆí‹°

- [GitHub Issues](https://github.com/QuentinFuxa/WhisperLiveKit/issues)
- [Discussions](https://github.com/QuentinFuxa/WhisperLiveKit/discussions)

---

**ë¬¸ì„œ ë²„ì „**: 1.0.0
**ìµœì¢… ì—…ë°ì´íŠ¸**: 2026-01-26
**ì‘ì„±ì**: WhisperLiveKit ì»¤ë®¤ë‹ˆí‹°
