# WhisperLiveKit íŒŒì¼ë³„ í•œêµ­ì–´ ì£¼ì„ ê°€ì´ë“œ

ì´ ë¬¸ì„œëŠ” WhisperLiveKit í”„ë¡œì íŠ¸ì˜ ê° íŒŒì¼ì— ëŒ€í•œ í•œêµ­ì–´ ì„¤ëª…ê³¼ ì£¼ìš” í•¨ìˆ˜/í´ë˜ìŠ¤ì˜ ì—­í• ì„ ì •ë¦¬í•œ ê²ƒì…ë‹ˆë‹¤.

---

## ğŸ“ ì½”ì–´ ëª¨ë“ˆ (whisperlivekit/)

### `__init__.py`
**ì—­í• **: íŒ¨í‚¤ì§€ ì§„ì…ì , ì£¼ìš” í´ë˜ìŠ¤/í•¨ìˆ˜ export

**ì£¼ìš” export**:
```python
from .audio_processor import AudioProcessor  # ì˜¤ë””ì˜¤ ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸
from .core import TranscriptionEngine  # ì „ì‚¬ ì—”ì§„ (ì‹±ê¸€í†¤)
from .parse_args import parse_args  # CLI ì¸ì íŒŒì„œ
from .web.web_interface import get_inline_ui_html, get_web_interface_html  # ì›¹ UI
```

---

### `core.py` â­ (í•µì‹¬ ì—”ì§„)
**ì—­í• **: ëª¨ë“  AI ëª¨ë¸ì„ ì´ˆê¸°í™”í•˜ê³  ê´€ë¦¬í•˜ëŠ” ì‹±ê¸€í†¤ í´ë˜ìŠ¤

#### ì£¼ìš” í´ë˜ìŠ¤: `TranscriptionEngine`
**íŒ¨í„´**: ìŠ¤ë ˆë“œ ì•ˆì „ ì‹±ê¸€í†¤ (Double-Checked Locking)

**ì´ˆê¸°í™” íŒŒë¼ë¯¸í„°**:
- `model_size`: ëª¨ë¸ í¬ê¸° (base, small, medium, large-v3)
- `lan`: ì†ŒìŠ¤ ì–¸ì–´ (auto, en, fr, ko, ...)
- `backend_policy`: ì „ì‚¬ ì •ì±… (simulstreaming, localagreement)
- `backend`: ASR ë°±ì—”ë“œ (auto, mlx-whisper, faster-whisper, whisper)
- `diarization`: í™”ì ì‹ë³„ í™œì„±í™” (True/False)
- `target_language`: ë²ˆì—­ ëŒ€ìƒ ì–¸ì–´ (ë¹ˆ ë¬¸ìì—´ = ë¹„í™œì„±í™”)

**ë©¤ë²„ ë³€ìˆ˜**:
- `self.asr`: ASR ë°±ì—”ë“œ ì¸ìŠ¤í„´ìŠ¤
- `self.diarization_model`: í™”ì ì‹ë³„ ëª¨ë¸
- `self.translation_model`: ë²ˆì—­ ëª¨ë¸ (NLLW)
- `self.vac_session`: VAC ì„¸ì…˜ (Silero VAD ONNX)

**ì‚¬ìš© ì˜ˆ**:
```python
engine = TranscriptionEngine(
    model_size="medium",
    lan="ko",
    diarization=True
)
```

#### íŒ©í† ë¦¬ í•¨ìˆ˜

##### `online_factory(args, asr)`
**ì—­í• **: ê° WebSocket ì—°ê²°ë§ˆë‹¤ ìƒˆë¡œìš´ ASR ì˜¨ë¼ì¸ í”„ë¡œì„¸ì„œ ìƒì„±

**ë°˜í™˜**:
- `SimulStreamingOnlineProcessor`: SimulStreaming ì •ì±…ìš©
- `OnlineASRProcessor`: LocalAgreement ì •ì±…ìš©

##### `online_diarization_factory(args, diarization_backend)`
**ì—­í• **: ê° ì—°ê²°ë§ˆë‹¤ ìƒˆë¡œìš´ í™”ì ì‹ë³„ ì¸ìŠ¤í„´ìŠ¤ ìƒì„±

**ë°˜í™˜**:
- `SortformerDiarizationOnline`: Sortformer ë°±ì—”ë“œìš©
- `DiartDiarization`: Diart ë°±ì—”ë“œìš© (ê³µìœ  ì¸ìŠ¤í„´ìŠ¤)

##### `online_translation_factory(args, translation_model)`
**ì—­í• **: ê° ì—°ê²°ë§ˆë‹¤ ìƒˆë¡œìš´ ë²ˆì—­ ì¸ìŠ¤í„´ìŠ¤ ìƒì„±

**ë°˜í™˜**:
- `OnlineTranslation`: NLLW ê¸°ë°˜ ë²ˆì—­ í”„ë¡œì„¸ì„œ

---

### `audio_processor.py` â­ (ì˜¤ë””ì˜¤ íŒŒì´í”„ë¼ì¸)
**ì—­í• **: ì‹¤ì‹œê°„ ì˜¤ë””ì˜¤ ìŠ¤íŠ¸ë¦¼ ì²˜ë¦¬ ë° ê²°ê³¼ ì¡°í•©

#### ì£¼ìš” í´ë˜ìŠ¤: `AudioProcessor`

**ì´ˆê¸°í™” íŒŒë¼ë¯¸í„°**:
- `transcription_engine`: TranscriptionEngine ì¸ìŠ¤í„´ìŠ¤ (í•„ìˆ˜)

**ì£¼ìš” ë©¤ë²„ ë³€ìˆ˜**:
- `self.sample_rate`: ìƒ˜í”Œë ˆì´íŠ¸ (16000 Hz)
- `self.channels`: ì±„ë„ ìˆ˜ (1 = ëª¨ë…¸)
- `self.ffmpeg_manager`: FFmpeg ê´€ë¦¬ì (WebM/Opus â†’ PCM ë³€í™˜)
- `self.vac`: Silero VAD ì¸ìŠ¤í„´ìŠ¤ (ìŒì„± í™œë™ ê°ì§€)
- `self.transcription_queue`: ì „ì‚¬ìš© asyncio.Queue
- `self.diarization_queue`: í™”ì ì‹ë³„ìš© asyncio.Queue
- `self.translation_queue`: ë²ˆì—­ìš© asyncio.Queue
- `self.state`: í˜„ì¬ ìƒíƒœ (State ê°ì²´)

**ì£¼ìš” ë©”ì„œë“œ**:

##### `async def create_tasks()`
**ì—­í• **: ëª¨ë“  ë¹„ë™ê¸° ì²˜ë¦¬ ì‘ì—… ì‹œì‘

**ë°˜í™˜**: `results_formatter()` ì œë„ˆë ˆì´í„°

**ìƒì„±ë˜ëŠ” ì‘ì—…**:
1. `ffmpeg_stdout_reader()`: FFmpeg ì¶œë ¥ ì½ê¸°
2. `transcription_processor()`: ASR ì²˜ë¦¬
3. `diarization_processor()`: í™”ì ì‹ë³„ ì²˜ë¦¬
4. `translation_processor()`: ë²ˆì—­ ì²˜ë¦¬
5. `watchdog()`: ì‘ì—… ê±´ê°• ëª¨ë‹ˆí„°ë§

##### `async def process_audio(message: bytes)`
**ì—­í• **: WebSocketì—ì„œ ë°›ì€ ì˜¤ë””ì˜¤ ë©”ì‹œì§€ ì²˜ë¦¬

**ì²˜ë¦¬ íë¦„**:
1. PCM ëª¨ë“œ: `self.pcm_buffer.extend(message)` â†’ `handle_pcm_data()`
2. ì••ì¶• ëª¨ë“œ: `ffmpeg_manager.write_data(message)` â†’ FFmpeg ë³€í™˜

##### `async def handle_pcm_data()`
**ì—­í• **: PCM ë°ì´í„° VAD ì²˜ë¦¬ ë° í ì‚½ì…

**ì²˜ë¦¬ ë‹¨ê³„**:
1. PCM ë²„í¼ â†’ NumPy ë°°ì—´ ë³€í™˜
2. Silero VAD ì‹¤í–‰ â†’ ìŒì„±/ì¹¨ë¬µ ê°ì§€
3. ìŒì„± í™œë™ ì¤‘ì´ë©´ `transcription_queue`ì— ì‚½ì…
4. ì¹¨ë¬µ ê°ì§€ ì‹œ `Silence` ê°ì²´ ì‚½ì…

##### `async def transcription_processor()`
**ì—­í• **: ASR ë°±ì—”ë“œ í˜¸ì¶œ ë° í† í° ìƒì„±

**ì²˜ë¦¬ íë¦„**:
1. íì—ì„œ ì˜¤ë””ì˜¤ ì²­í¬ ë˜ëŠ” Silence ê°ì²´ ê°€ì ¸ì˜¤ê¸°
2. ASR ì‹¤í–‰: `self.transcription.process_iter()`
3. ìƒˆ í† í° ìƒì„±: `ASRToken(start, end, text)`
4. ìƒíƒœ ì—…ë°ì´íŠ¸: `self.state.tokens.extend(new_tokens)`
5. ë²ˆì—­ íì— í† í° ì „ë‹¬

##### `async def results_formatter()`
**ì—­í• **: ì²˜ë¦¬ ê²°ê³¼ë¥¼ í”„ë¡ íŠ¸ì—”ë“œ í˜•ì‹ìœ¼ë¡œ ë³€í™˜

**ë°˜í™˜**: `FrontData` ê°ì²´ ìŠ¤íŠ¸ë¦¼

**ì¶œë ¥ êµ¬ì¡°**:
```python
{
    "status": "active_transcription",
    "lines": [Segment, ...],  # ì •ë ¬ëœ ì„¸ê·¸ë¨¼íŠ¸
    "buffer_transcription": "í˜„ì¬ ë²„í¼ í…ìŠ¤íŠ¸",
    "buffer_diarization": "í™”ìë³„ ë²„í¼",
    "buffer_translation": "ë²ˆì—­ ë²„í¼",
    "remaining_time_transcription": 0.5,  # ë‚¨ì€ ì²˜ë¦¬ ì‹œê°„
    "remaining_time_diarization": 0.2
}
```

---

### `basic_server.py` (FastAPI ì„œë²„)
**ì—­í• **: HTTP/WebSocket ì„œë²„ ì œê³µ

#### ì£¼ìš” ì—”ë“œí¬ì¸íŠ¸

##### `GET /`
**ì—­í• **: ì›¹ UI HTML ë°˜í™˜

**ì‘ë‹µ**: `get_inline_ui_html()` - ì¸ë¼ì¸ CSS/JSê°€ í¬í•¨ëœ HTML

##### `WebSocket /asr`
**ì—­í• **: ì‹¤ì‹œê°„ ì˜¤ë””ì˜¤ ìŠ¤íŠ¸ë¦¬ë° ë° ì „ì‚¬ ê²°ê³¼ ì „ì†¡

**ì²˜ë¦¬ íë¦„**:
1. WebSocket ì—°ê²° ìˆ˜ë½
2. `AudioProcessor` ì¸ìŠ¤í„´ìŠ¤ ìƒì„± (ì—°ê²°ë³„)
3. `create_tasks()` í˜¸ì¶œ â†’ ë¹„ë™ê¸° ì‘ì—… ì‹œì‘
4. `handle_websocket_results()` ì‘ì—… ìƒì„± â†’ ê²°ê³¼ ì „ì†¡
5. ë£¨í”„: `receive_bytes()` â†’ `process_audio(message)`
6. ì—°ê²° ì¢…ë£Œ ì‹œ: `cleanup()`

**ë©”ì‹œì§€ í˜•ì‹**:
- **ì…ë ¥**: Binary (WebM/Opus ë˜ëŠ” PCM s16le)
- **ì¶œë ¥**: JSON (FrontData.to_dict())

##### `async def lifespan(app)`
**ì—­í• **: ì„œë²„ ì‹œì‘/ì¢…ë£Œ ì‹œ ì‹¤í–‰

**ì‹œì‘ ì‹œ**: `TranscriptionEngine` ì´ˆê¸°í™” (ì‹±ê¸€í†¤)
**ì¢…ë£Œ ì‹œ**: ì •ë¦¬ ì‘ì—… (ì„ íƒ)

---

### `parse_args.py` (CLI ì¸ì íŒŒì„œ)
**ì—­í• **: 100ê°œ ì´ìƒì˜ ëª…ë ¹ì¤„ ì˜µì…˜ ì •ì˜

**ì£¼ìš” ì¸ì ê·¸ë£¹**:

#### ì„œë²„ ì„¤ì •
```bash
--host localhost          # ì„œë²„ í˜¸ìŠ¤íŠ¸
--port 8000               # ì„œë²„ í¬íŠ¸
--ssl-certfile cert.pem   # SSL ì¸ì¦ì„œ
--ssl-keyfile key.pem     # SSL í‚¤
```

#### ëª¨ë¸ ì„¤ì •
```bash
--model base              # ëª¨ë¸ í¬ê¸°
--model-path /path        # ì»¤ìŠ¤í…€ ëª¨ë¸ ê²½ë¡œ
--lora-path /path         # LoRA ì–´ëŒ‘í„°
--language en             # ì†ŒìŠ¤ ì–¸ì–´
--target-language ko      # ë²ˆì—­ ëŒ€ìƒ
```

#### ë°±ì—”ë“œ ì„¤ì •
```bash
--backend-policy simulstreaming    # ì „ì‚¬ ì •ì±…
--backend auto                     # ASR ë°±ì—”ë“œ
--diarization                      # í™”ì ì‹ë³„ í™œì„±í™”
--diarization-backend sortformer   # í™”ì ì‹ë³„ ë°±ì—”ë“œ
```

#### SimulStreaming ì˜µì…˜
```bash
--frame-threshold 25      # AlignAtt ì„ê³„ê°’
--beams 1                 # ë¹” íƒìƒ‰ í¬ê¸°
--audio-max-len 30.0      # ìµœëŒ€ ì˜¤ë””ì˜¤ ê¸¸ì´
--cif-ckpt-path /path     # CIF ëª¨ë¸ ê²½ë¡œ
```

**ë°˜í™˜**: `argparse.Namespace` ê°ì²´

---

### `ffmpeg_manager.py` (FFmpeg ê´€ë¦¬ì)
**ì—­í• **: ë¹„ë™ê¸° FFmpeg ì„œë¸Œí”„ë¡œì„¸ìŠ¤ ê´€ë¦¬

#### ì£¼ìš” í´ë˜ìŠ¤: `FFmpegManager`

**ì´ˆê¸°í™” íŒŒë¼ë¯¸í„°**:
- `sample_rate`: ì¶œë ¥ ìƒ˜í”Œë ˆì´íŠ¸ (16000)
- `channels`: ì¶œë ¥ ì±„ë„ ìˆ˜ (1)

**ì£¼ìš” ë©”ì„œë“œ**:

##### `async def start()`
**ì—­í• **: FFmpeg ì„œë¸Œí”„ë¡œì„¸ìŠ¤ ì‹œì‘

**FFmpeg ëª…ë ¹ì–´**:
```bash
ffmpeg -hide_banner -loglevel error -i pipe:0 \
  -f s16le -acodec pcm_s16le -ac 1 -ar 16000 pipe:1
```

**ì„¤ëª…**:
- `pipe:0`: stdinì—ì„œ ì…ë ¥ (WebM/Opus ë“±)
- `-f s16le`: PCM signed 16-bit little-endian ì¶œë ¥
- `-ac 1`: ëª¨ë…¸ ë³€í™˜
- `-ar 16000`: 16kHz ë¦¬ìƒ˜í”Œë§
- `pipe:1`: stdoutìœ¼ë¡œ ì¶œë ¥

##### `async def write_data(data: bytes)`
**ì—­í• **: FFmpeg stdinì— ì••ì¶• ì˜¤ë””ì˜¤ ì“°ê¸°

##### `async def read_data(size: int)`
**ì—­í• **: FFmpeg stdoutì—ì„œ PCM ë°ì´í„° ì½ê¸°

**ë°˜í™˜**: `bytes` (PCM s16le)

##### `async def stop()`
**ì—­í• **: FFmpeg ì¢…ë£Œ ë° ë¦¬ì†ŒìŠ¤ ì •ë¦¬

---

### `silero_vad_iterator.py` (ìŒì„± í™œë™ ê°ì§€)
**ì—­í• **: Silero VAD ëª¨ë¸ì„ ì‚¬ìš©í•œ ìŒì„±/ì¹¨ë¬µ ê°ì§€

#### ì£¼ìš” í´ë˜ìŠ¤

##### `OnnxSession`
**ì—­í• **: ê³µìœ  ONNX ì„¸ì…˜ (ìƒíƒœ ì—†ìŒ)

**ì¥ì **: ì—¬ëŸ¬ ì—°ê²°ì´ ë™ì¼í•œ ì„¸ì…˜ ê³µìœ  â†’ ë©”ëª¨ë¦¬ ì ˆì•½

##### `OnnxWrapper`
**ì—­í• **: ONNX ëŸ°íƒ€ì„ ë˜í¼ (ì—°ê²°ë³„ ìƒíƒœ ê´€ë¦¬)

**ë©¤ë²„ ë³€ìˆ˜**:
- `_shared_session`: ê³µìœ  OnnxSession
- `_state`: ë‚´ë¶€ ìƒíƒœ í…ì„œ [2, batch, 128]
- `_context`: ì»¨í…ìŠ¤íŠ¸ ìœˆë„ìš° (64 í”„ë ˆì„)

##### `FixedVADIterator(VADIterator)`
**ì—­í• **: ê°€ë³€ ê¸¸ì´ ì˜¤ë””ì˜¤ ì²­í¬ ì²˜ë¦¬

**ë©”ì„œë“œ**: `__call__(self, x)`
**ì…ë ¥**: NumPy ë°°ì—´ (ê°€ë³€ ê¸¸ì´)
**ì¶œë ¥**:
- `{"start": sample_index}`: ìŒì„± ì‹œì‘
- `{"end": sample_index}`: ìŒì„± ì¢…ë£Œ
- `None`: ë³€í™” ì—†ìŒ

**ë‚´ë¶€ ë™ì‘**:
1. ë²„í¼ì— ì˜¤ë””ì˜¤ ëˆ„ì 
2. 512 ìƒ˜í”Œë§ˆë‹¤ VAD ëª¨ë¸ ì‹¤í–‰
3. ìŒì„± í™•ë¥  > 0.5 â†’ ìŒì„±
4. ìŒì„± í™•ë¥  < 0.35 â†’ ì¹¨ë¬µ

---

### `timed_objects.py` (ë°ì´í„° êµ¬ì¡°)
**ì—­í• **: ì‹œê°„ ì •ë³´ê°€ ìˆëŠ” ë°ì´í„° í´ë˜ìŠ¤ ì •ì˜

#### ì£¼ìš” í´ë˜ìŠ¤

##### `ASRToken`
**ì—­í• **: ê°œë³„ ë‹¨ì–´/í† í° (íƒ€ì„ìŠ¤íƒ¬í”„ í¬í•¨)

**ì†ì„±**:
```python
start: float        # ì‹œì‘ ì‹œê°„ (ì´ˆ)
end: float          # ì¢…ë£Œ ì‹œê°„ (ì´ˆ)
text: str           # í…ìŠ¤íŠ¸
speaker: int        # í™”ì ID (-1 = ë¯¸ì§€ì •)
detected_language: str  # ê°ì§€ëœ ì–¸ì–´
```

##### `Segment`
**ì—­í• **: ì—¬ëŸ¬ í† í°ì˜ ì§‘í•© (ë¬¸ì¥/êµ¬ì ˆ)

**ì†ì„±**:
```python
start: float
end: float
text: str           # ì—°ê²°ëœ í…ìŠ¤íŠ¸
speaker: int        # í™”ì ID
translation: str    # ë²ˆì—­ (ì„ íƒ)
```

##### `Silence`
**ì—­í• **: ì¹¨ë¬µ êµ¬ê°„ í‘œì‹œ

**ì†ì„±**:
```python
start: float
end: float
duration: float
is_starting: bool   # ì¹¨ë¬µ ì‹œì‘ ì¤‘
has_ended: bool     # ì¹¨ë¬µ ì¢…ë£Œë¨
```

##### `FrontData`
**ì—­í• **: í”„ë¡ íŠ¸ì—”ë“œë¡œ ì „ì†¡ë  ì‘ë‹µ ë°ì´í„°

**ì†ì„±**:
```python
status: str         # "active_transcription", "no_audio_detected", "error"
lines: List[Segment]  # í™•ì •ëœ ì„¸ê·¸ë¨¼íŠ¸
buffer_transcription: str  # ì „ì‚¬ ë²„í¼
buffer_diarization: str    # í™”ì ì‹ë³„ ë²„í¼
buffer_translation: str    # ë²ˆì—­ ë²„í¼
remaining_time_transcription: float
remaining_time_diarization: float
```

##### `State`
**ì—­í• **: ì˜¤ë””ì˜¤ ì²˜ë¦¬ ìƒíƒœ ê´€ë¦¬

**ì˜êµ¬ ìƒíƒœ**:
```python
tokens: List[ASRToken]          # ëª¨ë“  í† í°
buffer_transcription: Transcript  # ì „ì‚¬ ë²„í¼
end_buffer: float                 # ë²„í¼ ì¢…ë£Œ ì‹œê°„
end_attributed_speaker: float     # í™”ì ì‹ë³„ ì¢…ë£Œ ì‹œê°„
```

**ì„ì‹œ ì—…ë°ì´íŠ¸ ë²„í¼**:
```python
new_tokens: List[ASRToken]       # ìƒˆ í† í° (TokensAlignmentê°€ ì†Œë¹„)
new_diarization: List[SpeakerSegment]
new_translation: List[Translation]
new_tokens_buffer: Transcript
new_translation_buffer: Translation
```

---

### `tokens_alignment.py` (í† í° ì •ë ¬)
**ì—­í• **: ì „ì‚¬, í™”ì ì‹ë³„, ë²ˆì—­ì„ ì‹œê°„ì¶•ìœ¼ë¡œ ì •ë ¬

#### ì£¼ìš” í´ë˜ìŠ¤: `TokensAlignment`

**ì´ˆê¸°í™” íŒŒë¼ë¯¸í„°**:
- `state`: State ê°ì²´
- `args`: ì„¤ì • íŒŒë¼ë¯¸í„°
- `sep`: ë‹¨ì–´ êµ¬ë¶„ì (ê¸°ë³¸ê°’: " ")

**ì£¼ìš” ë©”ì„œë“œ**:

##### `update()`
**ì—­í• **: stateì˜ ìƒˆ ë°ì´í„°ë¥¼ ë‚´ë¶€ ë²„í¼ë¡œ ì´ë™

**ì²˜ë¦¬**:
```python
self.new_tokens = state.new_tokens
state.new_tokens = []  # ë¹„ìš°ê¸°

self.all_tokens.extend(self.new_tokens)
self.all_diarization_segments.extend(self.new_diarization)
```

##### `get_lines(diarization, translation, current_silence)`
**ì—­í• **: ì •ë ¬ëœ ì„¸ê·¸ë¨¼íŠ¸ ìƒì„±

**ë°˜í™˜**:
- `segments: List[Segment]`: í™”ì/ë²ˆì—­ì´ ë§¤í•‘ëœ ì„¸ê·¸ë¨¼íŠ¸
- `diarization_buffer: str`: í™”ì ë²„í¼ í…ìŠ¤íŠ¸
- `translation_buffer: str`: ë²ˆì—­ ë²„í¼ í…ìŠ¤íŠ¸

**ì²˜ë¦¬ íë¦„**:
1. êµ¬ë‘ì ìœ¼ë¡œ í† í° ê·¸ë£¹í™”: `compute_punctuations_segments()`
2. í™”ì ì •ë³´ ë§¤í•‘: `intersection_duration()` ì‚¬ìš©
3. ë²ˆì—­ ì •ë³´ ë³‘í•©: `add_translation()`
4. Segment ê°ì²´ ìƒì„±

---

## ğŸ“ SimulStreaming ë°±ì—”ë“œ (whisperlivekit/simul_whisper/)

### `simul_whisper.py` â­ (AlignAtt ë””ì½”ë”)
**ì—­í• **: Attention Alignmentë¥¼ ì‚¬ìš©í•œ ë™ì‹œ ìŠ¤íŠ¸ë¦¬ë° ë””ì½”ë”©

#### ì£¼ìš” í´ë˜ìŠ¤: `AlignAtt`

**ì´ˆê¸°í™” íŒŒë¼ë¯¸í„°**:
- `model`: Whisper ëª¨ë¸ ì¸ìŠ¤í„´ìŠ¤
- `alignment_heads`: ì •ë ¬ í—¤ë“œ ë¦¬ìŠ¤íŠ¸ [(layer, head), ...]
- `frame_threshold`: ë°œí™” ì„ê³„ê°’ (ê¸°ë³¸ê°’: 25)
- `tokenizer`: í† í¬ë‚˜ì´ì € ì¸ìŠ¤í„´ìŠ¤

**ì£¼ìš” ë©”ì„œë“œ**:

##### `compute_alignment(encoder_output, tokens)`
**ì—­í• **: Cross-Attention ê°€ì¤‘ì¹˜ ê³„ì‚°

**ë°˜í™˜**: `[num_tokens, num_frames]` í…ì„œ

**ì²˜ë¦¬**:
1. ê° alignment headì˜ attention ê°€ì¤‘ì¹˜ ì¶”ì¶œ
2. í‰ê·  ê³„ì‚°
3. í˜„ì¬ í† í°ì´ ì–´ëŠ ì˜¤ë””ì˜¤ í”„ë ˆì„ì— ì§‘ì¤‘í•˜ëŠ”ì§€ ë°˜í™˜

##### `should_fire(alignment_scores)`
**ì—­í• **: ë°œí™” ì§€ì  ê°ì§€

**ì•Œê³ ë¦¬ì¦˜**:
```python
last_token_attention = alignment_scores[-1]  # ë§ˆì§€ë§‰ í† í°
recent_frames_attention = last_token_attention[-threshold:]
if recent_frames_attention.sum() > 0.25:  # 25% ì„ê³„ê°’
    return True  # ë‹¨ì–´ ì™„ì„±, ë°œí™”!
```

##### `decode_streaming(encoder_output, audio_chunk)`
**ì—­í• **: ìŠ¤íŠ¸ë¦¬ë° ë””ì½”ë”© ì‹¤í–‰

**ë°˜í™˜**: `ASRToken` ë˜ëŠ” `None`

**ì²˜ë¦¬ íë¦„**:
1. ë””ì½”ë” forward: `logits = model.decoder(...)`
2. Attention alignment ê³„ì‚°
3. `should_fire()` ì²´í¬
4. ë°œí™” ì‹œ:
   - ë‹¤ìŒ í† í° ìƒ˜í”Œë§
   - ASRToken ìƒì„±
   - KV-cache ì—…ë°ì´íŠ¸
5. ë¯¸ë°œí™” ì‹œ: `None` ë°˜í™˜

---

### `backend.py` (SimulStreamingASR)
**ì—­í• **: AlignAttë¥¼ ì‚¬ìš©í•œ ASR ë°±ì—”ë“œ ì„¤ì •

#### ì£¼ìš” í´ë˜ìŠ¤: `SimulStreamingASR`

**ì´ˆê¸°í™” íŒŒë¼ë¯¸í„°**:
- `model_size`: ëª¨ë¸ í¬ê¸°
- `lan`: ì–¸ì–´
- `backend`: "auto", "mlx-whisper", "faster-whisper", "whisper"
- `frame_threshold`: AlignAtt ì„ê³„ê°’
- `beams`: ë¹” íƒìƒ‰ í¬ê¸°

**ì£¼ìš” ë©”ì„œë“œ**:

##### `__init__(...)`
**ì—­í• **: ëª¨ë¸ ë° ì¸ì½”ë” ë¡œë“œ

**ì²˜ë¦¬ íë¦„**:
1. ë°±ì—”ë“œ ìë™ ì„ íƒ (`backend="auto"`)
   - macOS + ARM â†’ MLX-Whisper
   - ê¸°íƒ€ â†’ Faster-Whisper â†’ PyTorch
2. Alignment Heads ë¡œë“œ
3. ì¸ì½”ë” ì„¤ì • (ë¹ ë¥¸ ì¸ì½”ë” ì‚¬ìš©)

##### `transcribe(audio)`
**ì—­í• **: ì˜¤ë””ì˜¤ ì „ì‚¬ (ë°°ì¹˜)

**ë‚´ë¶€ í˜¸ì¶œ**: `AlignAtt.decode_streaming()`

---

### `decoder_state.py` (ë””ì½”ë” ìƒíƒœ)
**ì—­í• **: ë””ì½”ë”ì˜ ìƒíƒœ ê´€ë¦¬ (í† í°, KV-ìºì‹œ)

#### ì£¼ìš” í´ë˜ìŠ¤: `DecoderState`

**ì†ì„±**:
```python
tokens: List[int]           # ìƒì„±ëœ í† í° ID
kv_cache: Dict[str, Tensor] # KV-ìºì‹œ (ë ˆì´ì–´ë³„)
audio_offset: float         # ì˜¤ë””ì˜¤ ì˜¤í”„ì…‹
```

**ì£¼ìš” ë©”ì„œë“œ**:

##### `clean_cache()`
**ì—­í• **: KV-ìºì‹œ ë©”ëª¨ë¦¬ í•´ì œ

**ì²˜ë¦¬**:
```python
for key in list(self.kv_cache.keys()):
    tensor = self.kv_cache.pop(key)
    del tensor

if torch.cuda.is_available():
    torch.cuda.empty_cache()
```

---

### `token_buffer.py` (í† í° ë²„í¼)
**ì—­í• **: ìƒì„±ëœ í† í°ì˜ ë²„í¼ ê´€ë¦¬

#### ì£¼ìš” í´ë˜ìŠ¤: `TokenBuffer`

**ë©”ì„œë“œ**:

##### `add_token(token_id, text, timing)`
**ì—­í• **: í† í° ì¶”ê°€

##### `get_confirmed_tokens()`
**ì—­í• **: í™•ì •ëœ í† í° ë°˜í™˜

##### `reset()`
**ì—­í• **: ë²„í¼ ì´ˆê¸°í™”

---

### `beam.py` (ë¹” íƒìƒ‰)
**ì—­í• **: ë¹” íƒìƒ‰ ë””ì½”ë”© êµ¬í˜„

#### ì£¼ìš” í´ë˜ìŠ¤: `BeamSearch`

**ì´ˆê¸°í™” íŒŒë¼ë¯¸í„°**:
- `beam_size`: ë¹” í¬ê¸° (ê¸°ë³¸ê°’: 5)
- `max_length`: ìµœëŒ€ ì‹œí€€ìŠ¤ ê¸¸ì´

**ì£¼ìš” ë©”ì„œë“œ**:

##### `search(model, encoder_output)`
**ì—­í• **: ë¹” íƒìƒ‰ ì‹¤í–‰

**ë°˜í™˜**: ìµœê³  ì ìˆ˜ í† í° ì‹œí€€ìŠ¤

**ì•Œê³ ë¦¬ì¦˜**:
1. ì´ˆê¸° ë¹”: [<SOT>]
2. ê° ë‹¨ê³„:
   - ê° ë¹”ì—ì„œ ìƒìœ„ Kê°œ í† í° ìƒì„±
   - ì´ KÃ—Kê°œ í›„ë³´ ì¤‘ ìƒìœ„ Kê°œ ì„ íƒ
3. ì¢…ë£Œ ì¡°ê±´: ëª¨ë“  ë¹”ì´ <EOT>ì— ë„ë‹¬
4. ìµœê³  ì ìˆ˜ ë¹” ë°˜í™˜

---

### `eow_detection.py` (ë‹¨ì–´ ë ê°ì§€)
**ì—­í• **: CIF (Continuous Integrate-and-Fire) êµ¬í˜„

#### ì£¼ìš” í´ë˜ìŠ¤: `CIFModel`

**ì—­í• **: Encoder ì¶œë ¥ì—ì„œ ë‹¨ì–´ ê²½ê³„ ì˜ˆì¸¡

**ë©”ì„œë“œ**:

##### `forward(encoder_output)`
**ì—­í• **: ê° í”„ë ˆì„ì˜ ë°œí™” í™•ë¥  ê³„ì‚°

**ë°˜í™˜**: `[num_frames]` í…ì„œ (0-1 í™•ë¥ )

**ì•Œê³ ë¦¬ì¦˜**:
```python
h_t = encoder_output[t]
Î±_t = sigmoid(W_cif @ h_t + b_cif)  # ë°œí™” í™•ë¥ 
C_t = cumsum(Î±)  # ëˆ„ì  í•©
if floor(C_t) > floor(C_{t-1}):
    fire = True  # ë‹¨ì–´ ê²½ê³„
```

---

## ğŸ“ LocalAgreement ë°±ì—”ë“œ (whisperlivekit/local_agreement/)

### `whisper_online.py` (ë°±ì—”ë“œ íŒ©í† ë¦¬)
**ì—­í• **: ASR ë°±ì—”ë“œ ì„ íƒ ë° ìƒì„±

#### ì£¼ìš” í•¨ìˆ˜: `backend_factory(backend, **kwargs)`

**ì§€ì› ë°±ì—”ë“œ**:
- `"whisper"`: PyTorch Whisper
- `"faster-whisper"`: CTranslate2 Faster-Whisper
- `"mlx-whisper"`: MLX-Whisper (Apple Silicon)
- `"openai-api"`: OpenAI API

**ë°˜í™˜**: ASR ë°±ì—”ë“œ ì¸ìŠ¤í„´ìŠ¤

---

### `online_asr.py` (ì˜¨ë¼ì¸ í”„ë¡œì„¸ì„œ)
**ì—­í• **: LocalAgreement ì •ì±… êµ¬í˜„

#### ì£¼ìš” í´ë˜ìŠ¤: `OnlineASRProcessor`

**ì´ˆê¸°í™” íŒŒë¼ë¯¸í„°**:
- `asr`: ASR ë°±ì—”ë“œ
- `buffer_trimming`: "sentence" ë˜ëŠ” "segment"

**ì£¼ìš” ë©”ì„œë“œ**:

##### `insert_audio_chunk(audio, offset)`
**ì—­í• **: ì˜¤ë””ì˜¤ ì²­í¬ ì‚½ì…

##### `process_iter()`
**ì—­í• **: ASR ì‹¤í–‰ ë° í† í° í™•ì •

**ë°˜í™˜**: `(confirmed_tokens, processed_upto)`

**ì²˜ë¦¬ íë¦„**:
1. ASR ì‹¤í–‰: `result = self.asr.transcribe(audio)`
2. íƒ€ì„ìŠ¤íƒ¬í”„ ë‹¨ì–´ ì¶”ì¶œ: `words = self.asr.ts_words(result)`
3. Hypothesis Bufferì— ì‚½ì…
4. LCP (Longest Common Prefix) ê³„ì‚°
5. í™•ì •ëœ í† í° ë°˜í™˜

##### `get_buffer()`
**ì—­í• **: í˜„ì¬ ë²„í¼ í…ìŠ¤íŠ¸ ë°˜í™˜

**ë°˜í™˜**: `Transcript` ê°ì²´

---

### `backends.py` (ASR ë°±ì—”ë“œ êµ¬í˜„)

#### `WhisperASR`
**ì—­í• **: PyTorch Whisper ë°±ì—”ë“œ

**ë©”ì„œë“œ**:
- `transcribe(audio)`: ì „ì‚¬ ì‹¤í–‰
- `ts_words(segments)`: íƒ€ì„ìŠ¤íƒ¬í”„ ë‹¨ì–´ ì¶”ì¶œ

#### `FasterWhisperASR`
**ì—­í• **: CTranslate2 ìµœì í™” ë°±ì—”ë“œ

**ì¥ì **: 4-10ë°° ë¹ ë¥¸ ì¶”ë¡ , INT8 ì–‘ìí™”

#### `MLXWhisper`
**ì—­í• **: Apple Silicon ìµœì í™” ë°±ì—”ë“œ

**ì¥ì **: ì´ˆì €ì§€ì—° (10-50ms), í†µí•© ë©”ëª¨ë¦¬ ì‚¬ìš©

#### `OpenaiApiASR`
**ì—­í• **: OpenAI API í´ë¼ìš°ë“œ ë°±ì—”ë“œ

**ì¥ì **: ë¬´ì œí•œ ë™ì‹œì„±, ìµœê³  ì •í™•ë„
**ë‹¨ì **: ë†’ì€ ì§€ì—°ì‹œê°„ (1-3ì´ˆ), ìœ ë£Œ

---

## ğŸ“ í™”ì ì‹ë³„ (whisperlivekit/diarization/)

### `sortformer_backend.py` â­ (Sortformer)
**ì—­í• **: ìŠ¤íŠ¸ë¦¬ë° í™”ì ì‹ë³„ (SOTA 2025)

#### ì£¼ìš” í´ë˜ìŠ¤: `SortformerDiarization`

**ì—­í• **: ê³µìœ  Sortformer ëª¨ë¸ (ì‹±ê¸€í†¤)

**ì´ˆê¸°í™”**:
```python
from nemo.collections.asr.models import SortformerEncLabelModel
self.model = SortformerEncLabelModel.from_pretrained("nvidia/sortformer-diar-1b")
```

#### ì£¼ìš” í´ë˜ìŠ¤: `SortformerDiarizationOnline`

**ì—­í• **: ì—°ê²°ë³„ ìŠ¤íŠ¸ë¦¬ë° ìƒíƒœ ê´€ë¦¬

**ë©”ì„œë“œ**:

##### `insert_audio_chunk(audio)`
**ì—­í• **: ì˜¤ë””ì˜¤ ì²­í¬ ì‚½ì…

##### `diarize()`
**ì—­í• **: í™”ì ì‹ë³„ ì‹¤í–‰

**ë°˜í™˜**: `List[SpeakerSegment]`

**ì²˜ë¦¬ íë¦„**:
1. Mel-spectrogram ì¶”ì¶œ (10 í”„ë ˆì„ ì²­í¬)
2. Sortformer ëª¨ë¸ forward
3. Speaker embeddings ìƒì„±
4. ì˜¨ë¼ì¸ í´ëŸ¬ìŠ¤í„°ë§
5. SpeakerSegment ì¶œë ¥

---

### `diart_backend.py` (Diart)
**ì—­í• **: Pyannote ê¸°ë°˜ í™”ì ì‹ë³„ (SOTA 2021)

#### ì£¼ìš” í´ë˜ìŠ¤: `DiartDiarization`

**ì—­í• **: RxPy Observable ê¸°ë°˜ í™”ì ì‹ë³„

**ì´ˆê¸°í™” íŒŒë¼ë¯¸í„°**:
- `segmentation_model`: "pyannote/segmentation-3.0"
- `embedding_model`: "speechbrain/spkrec-ecapa-voxceleb"

**ë©”ì„œë“œ**:

##### `start()`
**ì—­í• **: Observable ìŠ¤íŠ¸ë¦¼ ì‹œì‘

##### `insert_audio_chunk(audio)`
**ì—­í• **: ì˜¤ë””ì˜¤ ì‚½ì…

**ë‚´ë¶€ ë™ì‘**:
1. Segmentation: ìŒì„± í™œë™ ê°ì§€
2. Embedding: í™”ì ì„ë² ë”© ìƒì„±
3. Clustering: í™”ìë³„ ê·¸ë£¹í™”
4. Observable emit: SpeakerSegment

---

## ğŸ“ Whisper ëª¨ë¸ (whisperlivekit/whisper/)

### `model.py` (Whisper ì•„í‚¤í…ì²˜)
**ì—­í• **: Whisper íŠ¸ëœìŠ¤í¬ë¨¸ êµ¬í˜„

#### ì£¼ìš” í´ë˜ìŠ¤: `Whisper`

**êµ¬ì¡°**:
```python
Whisper
â”œâ”€â”€ encoder: AudioEncoder
â”‚   â”œâ”€â”€ Conv1d (80 â†’ 1280)
â”‚   â”œâ”€â”€ Conv1d (stride=2)
â”‚   â”œâ”€â”€ PositionalEmbedding
â”‚   â””â”€â”€ ResidualAttentionBlock Ã— 32
â””â”€â”€ decoder: TextDecoder
    â”œâ”€â”€ TokenEmbedding (51865 vocab)
    â”œâ”€â”€ PositionalEmbedding
    â””â”€â”€ ResidualAttentionBlock Ã— 32 (Cross-Attention)
```

**ì…ë ¥/ì¶œë ¥**:
- ì…ë ¥: Mel-spectrogram [80, 3000]
- ì¶œë ¥: Logits [seq_len, 51865]

---

### `audio.py` (ì˜¤ë””ì˜¤ ì „ì²˜ë¦¬)
**ì—­í• **: Mel-spectrogram ì¶”ì¶œ

#### ì£¼ìš” í•¨ìˆ˜

##### `log_mel_spectrogram(audio)`
**ì—­í• **: ì˜¤ë””ì˜¤ â†’ Mel-spectrogram ë³€í™˜

**íŒŒë¼ë¯¸í„°**:
- `N_FFT=400`: FFT ìœˆë„ìš° í¬ê¸°
- `HOP_LENGTH=160`: ìŠ¤íŠ¸ë¼ì´ë“œ
- `N_MELS=80`: Mel ë¹ˆ ê°œìˆ˜

**ë°˜í™˜**: `[80, frames]` í…ì„œ

---

### `decoding.py` (ë””ì½”ë”©)
**ì—­í• **: í† í° ìƒì„± (Greedy, Beam Search)

#### ì£¼ìš” í´ë˜ìŠ¤

##### `GreedyDecoder`
**ì—­í• **: Argmax ê¸°ë°˜ ë””ì½”ë”©

**ì¥ì **: ë¹ ë¦„ (1x)
**ë‹¨ì **: ì°¨ì„  ê²½ë¡œ íƒìƒ‰ ì•ˆ í•¨

##### `BeamSearchDecoder`
**ì—­í• **: ë¹” íƒìƒ‰ ë””ì½”ë”©

**ì¥ì **: ë†’ì€ ì •í™•ë„
**ë‹¨ì **: ëŠë¦¼ (5-10x)

**íŒŒë¼ë¯¸í„°**:
- `beam_size`: ë¹” í¬ê¸°
- `patience`: ì¡°ê¸° ì¢…ë£Œ ì¸ë‚´ì‹¬
- `length_penalty`: ê¸¸ì´ í˜ë„í‹°

---

### `tokenizer.py` (í† í¬ë‚˜ì´ì €)
**ì—­í• **: í…ìŠ¤íŠ¸ â†” í† í° ID ë³€í™˜

#### ì£¼ìš” í´ë˜ìŠ¤: `Tokenizer`

**ì§€ì› ì–¸ì–´**: 99ê°œ

**íŠ¹ìˆ˜ í† í°**:
- `<SOT>`: Start-of-Transcript
- `<EOT>`: End-of-Transcript
- `<|en|>`: ì–¸ì–´ í† í°
- `<|notimestamps|>`: íƒ€ì„ìŠ¤íƒ¬í”„ ë¹„í™œì„±í™”
- `<|0.00|>`: íƒ€ì„ìŠ¤íƒ¬í”„

**ë©”ì„œë“œ**:

##### `encode(text)`
**ì—­í• **: í…ìŠ¤íŠ¸ â†’ í† í° ID

##### `decode(token_ids)`
**ì—­í• **: í† í° ID â†’ í…ìŠ¤íŠ¸

---

### `transcribe.py` (ì „ì‚¬ íŒŒì´í”„ë¼ì¸)
**ì—­í• **: ê³ ìˆ˜ì¤€ ì „ì‚¬ ì¸í„°í˜ì´ìŠ¤

#### ì£¼ìš” í•¨ìˆ˜: `transcribe(model, audio, **kwargs)`

**ì²˜ë¦¬ íë¦„**:
1. ì˜¤ë””ì˜¤ â†’ Mel-spectrogram
2. 30ì´ˆ ì²­í¬ë¡œ ë¶„í• 
3. ì–¸ì–´ ê°ì§€ (lan="auto"ì¸ ê²½ìš°)
4. ê° ì²­í¬ ì „ì‚¬
5. ì„¸ê·¸ë¨¼íŠ¸ ë³‘í•©
6. íƒ€ì„ìŠ¤íƒ¬í”„ ì¶”ì¶œ
7. ê²°ê³¼ ë°˜í™˜

**ë°˜í™˜**:
```python
{
    "text": "ì „ì²´ í…ìŠ¤íŠ¸",
    "segments": [
        {
            "start": 0.0,
            "end": 3.5,
            "text": "Hello world",
            "words": [
                {"word": "Hello", "start": 0.0, "end": 0.6},
                {"word": "world", "start": 0.8, "end": 1.2}
            ]
        },
        ...
    ],
    "language": "en"
}
```

---

## ğŸ“ ì›¹ ì¸í„°í˜ì´ìŠ¤ (whisperlivekit/web/)

### `web_interface.py` (UI ë¡œë”)
**ì—­í• **: HTML/CSS/JS íŒŒì¼ ë¡œë“œ ë° ì¸ë¼ì¸í™”

#### ì£¼ìš” í•¨ìˆ˜

##### `get_inline_ui_html()`
**ì—­í• **: ë‹¨ì¼ HTML íŒŒì¼ ìƒì„± (CSS/JS ì¸ë¼ì¸)

**ë°˜í™˜**: ì™„ì „í•œ HTML ë¬¸ìì—´

**ì²˜ë¦¬**:
1. HTML íŒŒì¼ ì½ê¸°
2. CSS íŒŒì¼ ì½ê¸° â†’ `<style>` íƒœê·¸ë¡œ ì‚½ì…
3. JS íŒŒì¼ ì½ê¸° â†’ `<script>` íƒœê·¸ë¡œ ì‚½ì…
4. SVG ì•„ì´ì½˜ â†’ data URI ë³€í™˜
5. AudioWorklet/Worker â†’ ì¸ë¼ì¸ ì‚½ì…

---

### `live_transcription.html/css/js` (í”„ë¡ íŠ¸ì—”ë“œ)

#### HTML êµ¬ì¡°
```html
<div id="app">
    <div id="settings">
        <!-- ì„¤ì • íŒ¨ë„ -->
    </div>
    <div id="transcript">
        <!-- ì „ì‚¬ ì¶œë ¥ -->
    </div>
    <div id="controls">
        <button id="start">Start</button>
        <button id="stop">Stop</button>
    </div>
</div>
```

#### JavaScript ì£¼ìš” í•¨ìˆ˜

##### `startRecording()`
**ì—­í• **: ì˜¤ë””ì˜¤ ìº¡ì²˜ ì‹œì‘

**ì²˜ë¦¬**:
1. `navigator.mediaDevices.getUserMedia()` í˜¸ì¶œ
2. MediaRecorder ë˜ëŠ” AudioWorklet ì´ˆê¸°í™”
3. WebSocket ì—°ê²°: `ws://localhost:8000/asr`
4. ì˜¤ë””ì˜¤ ì²­í¬ ì „ì†¡

##### `onWebSocketMessage(event)`
**ì—­í• **: ì„œë²„ ì‘ë‹µ ì²˜ë¦¬

**ì²˜ë¦¬**:
```javascript
const data = JSON.parse(event.data);

if (data.type === "config") {
    // ì„¤ì • ìˆ˜ì‹ 
    useAudioWorklet = data.useAudioWorklet;
}

if (data.status === "active_transcription") {
    // ì „ì‚¬ ê²°ê³¼ í‘œì‹œ
    updateTranscript(data.lines);
    updateBuffer(data.buffer_transcription);
}
```

##### `updateTranscript(lines)`
**ì—­í• **: UI ì—…ë°ì´íŠ¸

**ì²˜ë¦¬**:
```javascript
lines.forEach(segment => {
    const div = document.createElement('div');
    div.className = 'segment';
    div.dataset.speaker = segment.speaker;
    div.textContent = segment.text;
    transcriptDiv.appendChild(div);
});
```

---

### `pcm_worklet.js` (AudioWorklet)
**ì—­í• **: PCM ëª¨ë“œ ì˜¤ë””ì˜¤ ìº¡ì²˜

#### í´ë˜ìŠ¤: `PCMProcessor`

**ì—­í• **: 16kHz PCM s16le ë°ì´í„° ìƒì„±

**ë©”ì„œë“œ**:

##### `process(inputs, outputs, parameters)`
**ì—­í• **: ì˜¤ë””ì˜¤ í”„ë ˆì„ ì²˜ë¦¬

**ì²˜ë¦¬**:
1. Float32 ì…ë ¥ â†’ Int16 ë³€í™˜
2. `postMessage()` â†’ ë©”ì¸ ìŠ¤ë ˆë“œë¡œ ì „ì†¡
3. WebSocketìœ¼ë¡œ ì „ì†¡

---

## ğŸ“ ìŠ¤í¬ë¦½íŠ¸ (scripts/)

### `convert_hf_whisper.py`
**ì—­í• **: HuggingFace ëª¨ë¸ â†’ WhisperLiveKit í˜•ì‹ ë³€í™˜

**ì‚¬ìš©ë²•**:
```bash
python scripts/convert_hf_whisper.py \
  --repo openai/whisper-large-v3 \
  --output ./models/large-v3
```

---

### `determine_alignment_heads.py`
**ì—­í• **: Alignment Heads ì¶”ì¶œ

**ì‚¬ìš©ë²•**:
```bash
python scripts/determine_alignment_heads.py \
  --model-path ./models/large-v3 \
  --output alignment_heads.json
```

**ì•Œê³ ë¦¬ì¦˜**:
1. ì—¬ëŸ¬ ì˜¤ë””ì˜¤ ìƒ˜í”Œ ì „ì‚¬
2. ê° í—¤ë“œì˜ attention ê°€ì¤‘ì¹˜ ë¶„ì„
3. Cross-Attentionì´ ê°•í•œ í—¤ë“œ ì„ íƒ
4. JSONìœ¼ë¡œ ì €ì¥

---

### `sync_extension.py`
**ì—­í• **: Chrome í™•ì¥ í”„ë¡œê·¸ë¨ ë™ê¸°í™”

**ì‚¬ìš©ë²•**:
```bash
python scripts/sync_extension.py --url http://localhost:8000
```

---

## ğŸ”§ ìœ í‹¸ë¦¬í‹° ëª¨ë“ˆ

### `backend_support.py`
**ì—­í• **: ë°±ì—”ë“œ ê°€ìš©ì„± í™•ì¸

#### í•¨ìˆ˜

##### `mlx_backend_available(warn_on_missing=False)`
**ì—­í• **: MLX-Whisper ì„¤ì¹˜ ì—¬ë¶€ í™•ì¸

**ì¡°ê±´**: macOS + ARM64 + mlx-whisper ì„¤ì¹˜

##### `faster_backend_available(warn_on_missing=False)`
**ì—­í• **: Faster-Whisper ì„¤ì¹˜ ì—¬ë¶€ í™•ì¸

---

### `model_paths.py`
**ì—­í• **: ëª¨ë¸ ê²½ë¡œ í•´ì„ ë° ë‹¤ìš´ë¡œë“œ

#### ì£¼ìš” í•¨ìˆ˜

##### `resolve_model_path(model_path)`
**ì—­í• **: ëª¨ë¸ ê²½ë¡œ â†’ ë¡œì»¬ ê²½ë¡œ ë³€í™˜

**ì²˜ë¦¬**:
1. ë¡œì»¬ íŒŒì¼ í™•ì¸
2. HuggingFace ì €ì¥ì†Œ í™•ì¸
3. ìë™ ë‹¤ìš´ë¡œë“œ
4. í˜•ì‹ ê°ì§€ (PyTorch, MLX, CTranslate2)

---

### `thread_safety.py`
**ì—­í• **: ìŠ¤ë ˆë“œ ì•ˆì „ ëª¨ë¸ ì ‘ê·¼

#### í•¨ìˆ˜

##### `@with_model_lock`
**ì—­í• **: ë°ì½”ë ˆì´í„° - ëª¨ë¸ í˜¸ì¶œ ì§ë ¬í™”

**ì‚¬ìš©ë²•**:
```python
@with_model_lock
def transcribe(audio):
    return model(audio)
```

---

### `warmup.py`
**ì—­í• **: ëª¨ë¸ ì›Œë°ì—…

#### í•¨ìˆ˜

##### `warmup_asr(asr, warmup_file=None)`
**ì—­í• **: ì²« í˜¸ì¶œ ì§€ì—° ê°ì†Œ

**ì²˜ë¦¬**:
1. JFK ìƒ˜í”Œ ë‹¤ìš´ë¡œë“œ
2. ASR ì‹¤í–‰ (ê²°ê³¼ ë¬´ì‹œ)
3. GPU ë©”ëª¨ë¦¬ í• ë‹¹
4. JIT ì»´íŒŒì¼ (PyTorch)

---

## ğŸ“Š ë°ì´í„° íë¦„ ìš”ì•½

```
[ë¸Œë¼ìš°ì €]
   â†“ WebSocket Binary
[basic_server.py - FastAPI]
   â†“ WebSocket.receive_bytes()
[AudioProcessor.process_audio()]
   â†“ FFmpeg ë³€í™˜ (ì„ íƒ)
[handle_pcm_data()]
   â†“ Silero VAD
[transcription_queue.put()]
   â†“
[transcription_processor()]
   â”œâ†’ SimulStreamingASR (AlignAtt)
   â””â†’ LocalAgreement (Hypothesis Buffer)
   â†“ ASRToken ìŠ¤íŠ¸ë¦¼
[state.tokens.extend()]
   â†“
[diarization_processor()] (ì„ íƒ)
   â””â†’ Sortformer / Diart
   â†“ SpeakerSegment ìŠ¤íŠ¸ë¦¼
[state.new_diarization.extend()]
   â†“
[translation_processor()] (ì„ íƒ)
   â””â†’ NLLW
   â†“ Translation ìŠ¤íŠ¸ë¦¼
[state.new_translation.extend()]
   â†“
[TokensAlignment.update()]
   â†“
[TokensAlignment.get_lines()]
   â†“ Segment ë¦¬ìŠ¤íŠ¸
[results_formatter()]
   â†“ FrontData.to_dict()
[WebSocket.send_json()]
   â†“
[ë¸Œë¼ìš°ì € UI ì—…ë°ì´íŠ¸]
```

---

## ğŸ¯ ì£¼ìš” ì„¤ê³„ íŒ¨í„´

### 1. ì‹±ê¸€í†¤ íŒ¨í„´
**ìœ„ì¹˜**: `TranscriptionEngine`
**ëª©ì **: ëª¨ë¸ ì¸ìŠ¤í„´ìŠ¤ ê³µìœ ë¡œ ë©”ëª¨ë¦¬ ì ˆì•½

### 2. íŒ©í† ë¦¬ íŒ¨í„´
**ìœ„ì¹˜**: `online_factory()`, `backend_factory()`
**ëª©ì **: ë°±ì—”ë“œ ì„ íƒ ì¶”ìƒí™”

### 3. Producer-Consumer íŒ¨í„´
**ìœ„ì¹˜**: asyncio.Queue (ì „ì‚¬, í™”ìì‹ë³„, ë²ˆì—­)
**ëª©ì **: ë¹„ë™ê¸° ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸

### 4. Observer íŒ¨í„´
**ìœ„ì¹˜**: Diart (RxPy Observable)
**ëª©ì **: ì´ë²¤íŠ¸ ê¸°ë°˜ í™”ì ì‹ë³„

### 5. State íŒ¨í„´
**ìœ„ì¹˜**: `FFmpegState`, `DecoderState`
**ëª©ì **: ìƒíƒœ ê´€ë¦¬ ë° ì „í™˜

---

## ğŸš€ ì„±ëŠ¥ ìµœì í™” ê¸°ë²•

### 1. KV-Cache
**ìœ„ì¹˜**: `decoder_state.py`
**íš¨ê³¼**: 10-100ë°° ë©”ëª¨ë¦¬/ê³„ì‚° ì ˆì•½

### 2. Attention Alignment
**ìœ„ì¹˜**: `simul_whisper.py`
**íš¨ê³¼**: 300-800ms ë‹¨ì–´ ë‹¨ìœ„ ì§€ì—°

### 3. ONNX Runtime
**ìœ„ì¹˜**: `silero_vad_iterator.py`
**íš¨ê³¼**: VAD 3-5ë°° ë¹ ë¦„

### 4. ë¹„ë™ê¸° I/O
**ìœ„ì¹˜**: `audio_processor.py`, `ffmpeg_manager.py`
**íš¨ê³¼**: ë‹¤ì¤‘ ì—°ê²° ë™ì‹œ ì²˜ë¦¬

### 5. ë¹ ë¥¸ ì¸ì½”ë”
**ìœ„ì¹˜**: Faster-Whisper, MLX-Whisper
**íš¨ê³¼**: 4-10ë°° ë¹ ë¥¸ ì¸ì½”ë”©

---

**ë¬¸ì„œ ì‘ì„±ì¼**: 2026-01-26
**ì‘ì„±ì**: WhisperLiveKit Korean Documentation Team
**ë²„ì „**: 0.2.17.post1
